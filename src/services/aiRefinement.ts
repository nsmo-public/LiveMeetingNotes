import type { TranscriptionResult } from '../types/types';

export interface RawTranscriptData {
  text: string;
  timestamp: string;
  audioTimeMs?: number;
  confidence: number;
  isFinal: boolean;
}

export interface RefinedSegment {
  text: string;
  timestamp: string;
  audioTimeMs?: number;
}

/**
 * AI Refinement Service for Gemini AI
 * Refines raw speech-to-text transcripts with AI
 */
export class AIRefinementService {
  private static readonly GEMINI_MODELS_ENDPOINT = 'https://generativelanguage.googleapis.com/v1beta/models';
  private static readonly GEMINI_API_VERSION = 'v1beta'; // Use v1beta as it's more stable

  // Gemini Free Tier Limits (per day)
  private static readonly FREE_TIER_LIMITS = {
    RPM: 15,           // Requests per minute
    TPM: 1000000,      // Tokens per minute (1M)
    RPD: 1500,         // Requests per day
    TPD: 250000        // Tokens per day (250K) - Main limit users hit
  };

  // Batch processing configuration
  private static readonly BATCH_SIZE = 30; // Reduced from 50 to 30 segments per batch (~5000 tokens)
  private static readonly BATCH_DELAY_MS = 6000; // Increased from 5000 to 6000ms (6 seconds) between batches to avoid rate limit

  /**
   * Estimate token count for transcripts
   * OPTIMIZED: Reduced prompt overhead after optimization (1000 -> 500 tokens)
   */
  private static estimateTokenCount(transcriptions: TranscriptionResult[]): number {
    // Rough estimation: 1 token ‚âà 4 characters for English, ~2-3 for Vietnamese
    const totalChars = transcriptions.reduce((sum, t) => sum + t.text.length, 0);
    // Vietnamese: ~2.5 chars per token, English: ~4 chars per token
    // Use 3 as average + reduced overhead for optimized prompt
    const estimatedTokens = Math.ceil(totalChars / 3) + 500; // +500 for compact prompt overhead (reduced from 1000)
    return estimatedTokens;
  }

  /**
   * Check if processing would exceed quota
   */
  private static checkQuotaEstimate(transcriptions: TranscriptionResult[]): {
    estimatedTokens: number;
    withinLimit: boolean;
    message: string;
  } {
    const estimatedTokens = this.estimateTokenCount(transcriptions);
    const withinLimit = estimatedTokens < this.FREE_TIER_LIMITS.TPD;

    let message = '';
    if (!withinLimit) {
      message = `‚ö†Ô∏è ∆Ø·ªõc t√≠nh ${estimatedTokens.toLocaleString()} tokens - v∆∞·ª£t h·∫°n m·ª©c mi·ªÖn ph√≠ (${this.FREE_TIER_LIMITS.TPD.toLocaleString()} tokens/ng√†y)`;
    } else {
      const percentUsed = Math.round((estimatedTokens / this.FREE_TIER_LIMITS.TPD) * 100);
      message = `‚úÖ ∆Ø·ªõc t√≠nh ${estimatedTokens.toLocaleString()} tokens (~${percentUsed}% h·∫°n m·ª©c mi·ªÖn ph√≠)`;
    }

    return { estimatedTokens, withinLimit, message };
  }

  /**
   * Split transcriptions into batches for processing
   */
  private static splitIntoBatches(transcriptions: TranscriptionResult[], batchSize: number): TranscriptionResult[][] {
    const batches: TranscriptionResult[][] = [];
    for (let i = 0; i < transcriptions.length; i += batchSize) {
      batches.push(transcriptions.slice(i, i + batchSize));
    }
    return batches;
  }

  /**
   * Check quota status by making a minimal test request
   * Returns usage info and recommendations
   */
  public static async checkQuotaStatus(apiKey: string, modelName: string): Promise<{
    status: 'available' | 'limited' | 'exceeded' | 'error';
    message: string;
    recommendations: string[];
  }> {
    try {
      const endpoint = `https://generativelanguage.googleapis.com/${this.GEMINI_API_VERSION}/${modelName}:generateContent`;
      
      const response = await fetch(`${endpoint}?key=${apiKey}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ parts: [{ text: 'test' }] }],
          generationConfig: { maxOutputTokens: 1 }
        })
      });
      
      if (response.status === 429) {
        const errorData = await response.json().catch(() => ({}));
        const errorMsg = errorData.error?.message || '';
        
        if (errorMsg.includes('quota') || errorMsg.includes('250000')) {
          return {
            status: 'exceeded',
            message: 'üö´ ƒê√£ v∆∞·ª£t h·∫°n m·ª©c 250,000 tokens/ng√†y',
            recommendations: [
              'üïí ƒê·ª£i 24 gi·ªù ƒë·ªÉ quota reset',
              'üí≥ N√¢ng c·∫•p Paid tier: ~$2/th√°ng, unlimited',
              'üìä Monitor: https://ai.dev/rate-limit'
            ]
          };
        } else {
          return {
            status: 'limited',
            message: '‚è±Ô∏è V∆∞·ª£t 15 requests/ph√∫t',
            recommendations: [
              '‚è∞ ƒê·ª£i 1-2 ph√∫t r·ªìi th·ª≠ l·∫°i',
              'üîÑ App s·∫Ω t·ª± ƒë·ªông delay gi·ªØa c√°c batch'
            ]
          };
        }
      } else if (response.ok) {
        return {
          status: 'available',
          message: '‚úÖ API Key ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng',
          recommendations: [
            'üéØ Free tier: 250,000 tokens/ng√†y',
            'üìä M·ªói 50 segments ~ 7,500 tokens',
            'üîç Monitor: https://ai.dev/rate-limit'
          ]
        };
      } else {
        return {
          status: 'error',
          message: `‚ùå L·ªói API: ${response.status}`,
          recommendations: [
            'Ki·ªÉm tra API Key c√≥ h·ª£p l·ªá',
            'Ki·ªÉm tra model ƒë√£ ch·ªçn ƒë√∫ng'
          ]
        };
      }
    } catch (error: any) {
      return {
        status: 'error',
        message: 'Kh√¥ng th·ªÉ ki·ªÉm tra quota',
        recommendations: [
          'Ki·ªÉm tra k·∫øt n·ªëi internet',
          'Th·ª≠ l·∫°i sau v√†i ph√∫t'
        ]
      };
    }
  }

  /**
   * Get usage metadata and quota information from Gemini API
   * Note: Gemini API doesn't provide direct quota endpoint, but we can infer from rate limit headers
   */
  public static async checkQuotaInfo(apiKey: string, modelName: string): Promise<{
    estimatedUsage: string;
    quotaStatus: string;
    recommendations: string[];
  }> {
    try {
      // Make a minimal test request to check quota status
      const endpoint = `https://generativelanguage.googleapis.com/${this.GEMINI_API_VERSION}/${modelName}:generateContent`;
      
      const response = await fetch(`${endpoint}?key=${apiKey}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          contents: [{
            parts: [{ text: 'test' }]
          }],
          generationConfig: {
            maxOutputTokens: 1
          }
        })
      });

      // Check response headers for quota info (if available)
      // const remainingRequests = response.headers.get('x-ratelimit-remaining');
      // const resetTime = response.headers.get('x-ratelimit-reset');
      
      // Parse response to check for quota errors
      const recommendations: string[] = [];
      let quotaStatus = 'unknown';
      let estimatedUsage = 'Kh√¥ng c√≥ th√¥ng tin chi ti·∫øt';

      if (response.status === 429) {
        quotaStatus = 'exceeded';
        const errorData = await response.json().catch(() => ({}));
        const errorMsg = errorData.error?.message || '';
        
        if (errorMsg.includes('quota')) {
          estimatedUsage = 'ƒê√£ v∆∞·ª£t h·∫°n m·ª©c 250,000 tokens/ng√†y';
          recommendations.push('ƒê·ª£i 24 gi·ªù ƒë·ªÉ quota reset');
          recommendations.push('Ho·∫∑c n√¢ng c·∫•p l√™n Paid tier (~$2/th√°ng)');
        } else {
          estimatedUsage = 'ƒê√£ v∆∞·ª£t 15 requests/ph√∫t';
          recommendations.push('ƒê·ª£i 1 ph√∫t r·ªìi th·ª≠ l·∫°i');
        }
      } else if (response.ok) {
        quotaStatus = 'available';
        estimatedUsage = 'API Key ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng';
        
        // Estimate based on typical usage
        recommendations.push('‚úÖ Free tier: 250,000 tokens/ng√†y, 15 requests/ph√∫t');
        recommendations.push('üí° M·ªói 50 segments ~ 7,500 tokens');
        recommendations.push('üìä Monitor: https://ai.dev/rate-limit');
      }

      return {
        estimatedUsage,
        quotaStatus,
        recommendations
      };
    } catch (error: any) {
      return {
        estimatedUsage: 'Kh√¥ng th·ªÉ ki·ªÉm tra quota',
        quotaStatus: 'error',
        recommendations: [
          'Ki·ªÉm tra API Key c√≥ h·ª£p l·ªá',
          'Ki·ªÉm tra k·∫øt n·ªëi internet',
          'Th·ª≠ l·∫°i sau v√†i ph√∫t'
        ]
      };
    }
  }

  /**
   * List available Gemini models for the given API key
   * Useful for debugging and verifying API key access
   */
  public static async listGeminiModels(apiKey: string): Promise<any> {
    if (!apiKey || apiKey.trim().length === 0) {
      throw new Error('API Key is required');
    }

    try {
      const response = await fetch(`${this.GEMINI_MODELS_ENDPOINT}?key=${apiKey}`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        }
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(
          `Failed to list models (${response.status}): ${errorData.error?.message || response.statusText}\n` +
          `URL: ${this.GEMINI_MODELS_ENDPOINT}`
        );
      }

      const data = await response.json();
      return data;
    } catch (error: any) {
      console.error('Error listing Gemini models:', error);
      throw new Error(`Cannot list Gemini models: ${error.message}`);
    }
  }

  /**
   * Refine transcripts using Gemini AI with automatic batching
   * @param transcriptions - Primary data (user-edited, highest reliability)
   * @param rawData - Supplementary data (original Web Speech API output for reference)
   */
  public static async refineTranscripts(
    apiKey: string,
    transcriptions: TranscriptionResult[], // Primary data source
    rawData: RawTranscriptData[], // Optional: supplementary raw data
    modelName: string, // REQUIRED: specific Gemini model (e.g., "models/gemini-2.5-flash")
    onProgress?: (progress: number) => void
  ): Promise<RefinedSegment[]> {
    // Check quota estimate first
    const quotaCheck = this.checkQuotaEstimate(transcriptions);
    console.log('üìä Quota Check:', quotaCheck.message);

    // If estimated tokens exceed limit, use batch processing
    if (quotaCheck.estimatedTokens > this.FREE_TIER_LIMITS.TPD * 0.8) { // 80% threshold
      console.log('üîÑ Using batch processing to avoid quota limits...');
      return this.refineTranscriptsInBatches(apiKey, transcriptions, rawData, modelName, onProgress);
    }

    // Otherwise, process normally
    return this.refineWithGemini(apiKey, transcriptions, rawData, modelName, onProgress);
  }

  /**
   * Refine transcripts in batches to avoid quota limits
   */
  private static async refineTranscriptsInBatches(
    apiKey: string,
    transcriptions: TranscriptionResult[],
    rawData: RawTranscriptData[],
    modelName: string,
    onProgress?: (progress: number) => void
  ): Promise<RefinedSegment[]> {
    const batches = this.splitIntoBatches(transcriptions, this.BATCH_SIZE);
    const allRefinedSegments: RefinedSegment[] = [];

    console.log(`üì¶ Processing ${transcriptions.length} segments in ${batches.length} batches...`);

    for (let i = 0; i < batches.length; i++) {
      const batch = batches[i];
      const batchProgress = (i / batches.length) * 100;

      console.log(`üîÑ Processing batch ${i + 1}/${batches.length} (${batch.length} segments)...`);

      try {
        // Find corresponding raw data for this batch
        const batchStartIndex = i * this.BATCH_SIZE;
        const batchRawData = rawData.slice(batchStartIndex, batchStartIndex + batch.length);

        // Process this batch
        const refinedBatch = await this.refineWithGemini(
          apiKey,
          batch,
          batchRawData,
          modelName,
          (subProgress) => {
            if (onProgress) {
              const totalProgress = batchProgress + (subProgress / batches.length);
              onProgress(Math.min(totalProgress, 99));
            }
          }
        );

        allRefinedSegments.push(...refinedBatch);

        // Add delay between batches to avoid rate limiting (except for last batch)
        if (i < batches.length - 1) {
          console.log(`‚è≥ Waiting ${this.BATCH_DELAY_MS / 1000} seconds before next batch to avoid rate limit...`);
          await new Promise(resolve => setTimeout(resolve, this.BATCH_DELAY_MS));
        }
      } catch (error: any) {
        // If quota exceeded, throw error with helpful message
        if (error.message.includes('429') || error.message.includes('quota')) {
          throw new Error(
            `V∆∞·ª£t h·∫°n m·ª©c API t·∫°i batch ${i + 1}/${batches.length}.\n\n` +
            `‚úÖ ƒê√£ x·ª≠ l√Ω: ${allRefinedSegments.length}/${transcriptions.length} segments\n\n` +
            `Nguy√™n nh√¢n: ${error.message}\n\n` +
            `üí° Gi·∫£i ph√°p:\n` +
            `‚Ä¢ ƒê·ª£i 24 gi·ªù ƒë·ªÉ quota reset (h·∫°n m·ª©c: 250,000 tokens/ng√†y)\n` +
            `‚Ä¢ Ho·∫∑c n√¢ng c·∫•p l√™n Gemini API tr·∫£ ph√≠ t·∫°i console.cloud.google.com`
          );
        }
        throw error;
      }
    }

    if (onProgress) onProgress(100);
    console.log(`‚úÖ Batch processing complete: ${allRefinedSegments.length} segments refined`);
    return allRefinedSegments;
  }

  /**
   * Refine with Google Gemini API using user-selected model
   */
  private static async refineWithGemini(
    apiKey: string,
    transcriptions: TranscriptionResult[], // Primary data
    rawData: RawTranscriptData[], // Supplementary data
    modelName: string, // REQUIRED: specific model like "models/gemini-2.5-flash"
    onProgress?: (progress: number) => void
  ): Promise<RefinedSegment[]> {
    if (!apiKey || apiKey.trim().length === 0) {
      throw new Error('API Key is required for AI refinement');
    }

    if (transcriptions.length === 0) {
      throw new Error('No transcript data to refine');
    }

    // Validate model name
    if (!modelName || !modelName.trim() || !modelName.startsWith('models/')) {
      throw new Error(
        'Vui l√≤ng ch·ªçn Gemini Model trong Settings.\n\n' +
        'B∆∞·ªõc 1: M·ªü Settings ‚Üí Nh·∫≠p Gemini API Key\n' +
        'B∆∞·ªõc 2: Ch·ªù h·ªá th·ªëng t·∫£i danh s√°ch models\n' +
        'B∆∞·ªõc 3: Ch·ªçn model t·ª´ dropdown (v√≠ d·ª•: Gemini 2.5 Flash)\n' +
        'B∆∞·ªõc 4: L∆∞u v√† th·ª≠ l·∫°i'
      );
    }

    try {
      // Prepare primary data from transcriptions (user-edited, highest reliability)
      // OPTIMIZED: Only send essential fields to reduce token usage
      const transcriptData = transcriptions.map((item) => ({
        timestamp: item.startTime,
        audioTimeMs: item.audioTimeMs,
        text: item.text
      }));

      // Prepare supplementary raw data (if available)
      // OPTIMIZED: Only send essential fields to reduce token usage
      const hasRawData = rawData && rawData.length > 0;
      const rawMetadata = hasRawData ? rawData.map((item) => ({
        timestamp: item.timestamp,
        audioTimeMs: item.audioTimeMs,
        text: item.text
      })) : null;

      // Create prompt
      const prompt = this.createRefinementPrompt(transcriptData, rawMetadata);

      if (onProgress) onProgress(10);

      // Build endpoint URL with selected model
      const endpoint = `https://generativelanguage.googleapis.com/${this.GEMINI_API_VERSION}/${modelName}:generateContent`;
      console.log(`ü§ñ Using Gemini model: ${modelName}`);
      console.log(`üì° Endpoint: ${endpoint}`);

      // Call Gemini API
      const response = await fetch(`${endpoint}?key=${apiKey}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          contents: [{
            parts: [{
              text: prompt
            }]
          }],
          generationConfig: {
            temperature: 0.2,
            topK: 40,
            topP: 0.95,
            maxOutputTokens: 8192,
          },
          safetySettings: [
            {
              category: "HARM_CATEGORY_HARASSMENT",
              threshold: "BLOCK_NONE"
            },
            {
              category: "HARM_CATEGORY_HATE_SPEECH",
              threshold: "BLOCK_NONE"
            },
            {
              category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              threshold: "BLOCK_NONE"
            },
            {
              category: "HARM_CATEGORY_DANGEROUS_CONTENT",
              threshold: "BLOCK_NONE"
            }
          ]
        })
      });

      if (onProgress) onProgress(70);

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const errorMsg = errorData.error?.message || response.statusText;
        
        // Handle quota/rate limit errors (429)
        if (response.status === 429) {
          // Extract retry time if available
          const retryMatch = errorMsg.match(/retry in ([\d.]+)s/);
          const retrySeconds = retryMatch ? Math.ceil(parseFloat(retryMatch[1])) : 60;
          const retryMinutes = Math.ceil(retrySeconds / 60);

          // Check if it's daily quota or rate limit
          if (errorMsg.includes('quota') || errorMsg.includes('250000')) {
            throw new Error(
              `üö´ ƒê√£ v∆∞·ª£t h·∫°n m·ª©c mi·ªÖn ph√≠ c·ªßa Gemini API\n\n` +
              `üìä H·∫°n m·ª©c free tier: 250,000 tokens/ng√†y\n` +
              `‚è∞ Th·ªùi gian reset: Sau ${retrySeconds}s (~ ${retryMinutes} ph√∫t)\n\n` +
              `üí° Gi·∫£i ph√°p:\n` +
              `1Ô∏è‚É£ ƒê·ª£i ${retryMinutes} ph√∫t r·ªìi th·ª≠ l·∫°i\n` +
              `2Ô∏è‚É£ X·ª≠ l√Ω √≠t segments h∆°n (ch·ªçn ƒëo·∫°n quan tr·ªçng ƒë·ªÉ chu·∫©n h√≥a)\n` +
              `3Ô∏è‚É£ N√¢ng c·∫•p l√™n Gemini API tr·∫£ ph√≠:\n` +
              `   ‚Ä¢ Truy c·∫≠p: https://console.cloud.google.com\n` +
              `   ‚Ä¢ Enable billing ƒë·ªÉ c√≥ quota cao h∆°n (60 requests/ph√∫t)\n\n` +
              `üìà Monitor usage: https://ai.dev/rate-limit\n\n` +
              `Chi ti·∫øt: ${errorMsg}`
            );
          } else {
            // Rate limit (RPM)
            throw new Error(
              `‚è±Ô∏è V∆∞·ª£t gi·ªõi h·∫°n requests/ph√∫t\n\n` +
              `üìä H·∫°n m·ª©c: 15 requests/ph√∫t (free tier)\n` +
              `‚è∞ Th·ª≠ l·∫°i sau: ${retrySeconds}s\n\n` +
              `üí° Gi·∫£i ph√°p: ƒê·ª£i ${Math.ceil(retrySeconds / 60)} ph√∫t r·ªìi th·ª≠ l·∫°i\n\n` +
              `Chi ti·∫øt: ${errorMsg}`
            );
          }
        }
        
        // Provide helpful error messages for other errors
        if (response.status === 403) {
          if (errorMsg.includes('API has not been used') || errorMsg.includes('SERVICE_DISABLED')) {
            throw new Error(
              'API Key kh√¥ng h·ª£p l·ªá ho·∫∑c ch∆∞a enable.\n\n' +
              '‚úÖ L·∫•y API key mi·ªÖn ph√≠ t·∫°i: https://aistudio.google.com/app/apikey\n' +
              'Sau ƒë√≥ paste v√†o Settings ‚Üí Gemini API Key'
            );
          } else if (errorMsg.includes('API_KEY_INVALID')) {
            throw new Error('API Key kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra l·∫°i trong Settings.');
          }
        } else if (response.status === 404) {
          throw new Error(
            `Model "${modelName}" kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng kh·∫£ d·ª•ng.\n\n` +
            'Gi·∫£i ph√°p:\n' +
            '1. M·ªü Settings ‚Üí Click n√∫t "T·∫£i l·∫°i" b√™n c·∫°nh Gemini Model\n' +
            '2. Ch·ªçn model kh√°c t·ª´ danh s√°ch (khuy√™n d√πng: Gemini 2.5 Flash)\n' +
            '3. L∆∞u v√† th·ª≠ l·∫°i\n\n' +
            `Chi ti·∫øt l·ªói: ${errorMsg}`
          );
        }
        
        throw new Error(`Gemini API error (${response.status}): ${errorMsg}`);
      }

      const result = await response.json();
      
      if (onProgress) onProgress(90);

      // Parse AI response
      const refinedSegments = this.parseAIResponse(result);

      if (onProgress) onProgress(100);

      console.log(`‚úÖ Successfully refined ${refinedSegments.length} segments`);
      return refinedSegments;

    } catch (error: any) {
      console.error('AI Refinement Error:', error);
      throw new Error(`Failed to refine transcripts: ${error.message}`);
    }
  }

  /**
   * Create refinement prompt for AI
   * @param transcriptData - Primary data from transcriptions (user-edited)
   * @param rawMetadata - Optional raw data for reference
   */
  private static createRefinementPrompt(transcriptData: any[], rawMetadata: any[] | null): string {
    // OPTIMIZED: Use compact JSON format (no pretty-print) to save tokens
    const dataJson = JSON.stringify(transcriptData);
    const hasRawData = rawMetadata && rawMetadata.length > 0;
    const rawDataJson = hasRawData ? JSON.stringify(rawMetadata) : null;

    // OPTIMIZED: Shortened prompt to reduce token count while maintaining quality
    return `Vai tr√≤: Th∆∞ k√Ω chuy√™n nghi·ªáp so·∫°n bi√™n b·∫£n h·ªçp.

Nhi·ªám v·ª•: Chu·∫©n h√≥a vƒÉn b·∫£n speech-to-text:
1. S·ª≠a l·ªói nh·∫≠n di·ªán t·ª´
2. X√≥a t·ª´ ƒë·ªám (√†, ·ª´m, th√¨, l√†, m√†)
3. Th√™m d·∫•u c√¢u, vi·∫øt hoa danh t·ª´ ri√™ng
4. G·ªôp c√°c ƒëo·∫°n li√™n ti·∫øp th√†nh c√¢u ho√†n ch·ªânh
5. Gi·ªØ nguy√™n n·ªôi dung, kh√¥ng th√™m b·ªõt √Ω

Output: CH·ªà JSON array, KH√îNG markdown/gi·∫£i th√≠ch
Format: [{"timestamp":"...","audioTimeMs":123,"text":"..."},...]

=== D·ªÆ LI·ªÜU CH√çNH ===
${dataJson}
${hasRawData ? `\n=== D·ªÆ LI·ªÜU B·ªî TR·ª¢ (tham kh·∫£o) ===\n${rawDataJson}` : ''}

Gi·ªØ timestamp/audioTimeMs g·ªëc. Ch·ªâ tr·∫£ v·ªÅ JSON array.`;
  }

  /**
   * Parse AI response and create refined segments
   */
  private static parseAIResponse(apiResponse: any): RefinedSegment[] {
    try {
      // Extract text from Gemini response
      const candidates = apiResponse.candidates;
      if (!candidates || candidates.length === 0) {
        throw new Error('No response from AI');
      }

      const content = candidates[0].content;
      if (!content || !content.parts || content.parts.length === 0) {
        throw new Error('Invalid AI response format');
      }

      let responseText = content.parts[0].text.trim();

      // Remove markdown code blocks if present
      responseText = responseText.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();

      // Log raw response for debugging
      console.log('üîç Raw AI response (first 500 chars):', responseText.substring(0, 500));

      // Try to extract JSON if there's additional text
      const jsonMatch = responseText.match(/\[[\s\S]*\]/);
      if (jsonMatch) {
        responseText = jsonMatch[0];
        console.log('‚úÇÔ∏è Extracted JSON array from response');
      }

      // Parse JSON with better error handling
      let refinedData;
      try {
        refinedData = JSON.parse(responseText);
      } catch (parseError: any) {
        console.error('‚ùå JSON Parse Error:', parseError.message);
        console.log('üìÑ Full response text:', responseText);
        
        // Try to fix common JSON issues
        let fixedText = responseText
          // Fix unescaped newlines in strings
          .replace(/"text"\s*:\s*"([^"]*?)"/g, (_match: string, text: string) => {
            const escaped = text
              .replace(/\n/g, '\\n')
              .replace(/\r/g, '\\r')
              .replace(/\t/g, '\\t');
            return `"text": "${escaped}"`;
          });

        console.log('üîß Attempting to fix JSON...');
        try {
          refinedData = JSON.parse(fixedText);
          console.log('‚úÖ JSON fixed and parsed successfully');
        } catch (secondError) {
          console.error('‚ùå Still cannot parse after fixes');
          throw parseError; // Throw original error
        }
      }

      if (!Array.isArray(refinedData)) {
        throw new Error('AI response is not an array');
      }

      // Validate and map to RefinedSegment
      const segments: RefinedSegment[] = refinedData
        .filter(item => item.text && item.text.trim().length > 0)
        .map(item => ({
          text: item.text.trim(),
          timestamp: item.timestamp || new Date().toISOString(),
          audioTimeMs: item.audioTimeMs
        }));

      return segments;

    } catch (error: any) {
      console.error('Failed to parse AI response:', error);
      console.log('Raw API response:', JSON.stringify(apiResponse, null, 2));
      throw new Error(`Failed to parse AI response: ${error.message}`);
    }
  }

  /**
   * Format audio time in milliseconds to mm:ss
   */
  // private static formatAudioTime(ms: number): string {
  //   const totalSeconds = Math.floor(ms / 1000);
  //   const minutes = Math.floor(totalSeconds / 60);
  //   const seconds = totalSeconds % 60;
  //   return `${minutes}:${String(seconds).padStart(2, '0')}`;
  // }

  /**
   * Convert refined segments back to TranscriptionResult format
   */
  public static convertToTranscriptionResults(
    refinedSegments: RefinedSegment[],
    speakerPrefix: string = 'Person1'
  ): TranscriptionResult[] {
    return refinedSegments.map((segment, index) => ({
      id: `refined-${Date.now()}-${index}`,
      text: segment.text,
      startTime: segment.timestamp,
      endTime: segment.timestamp, // Same as start for refined segments
      audioTimeMs: segment.audioTimeMs,
      confidence: 1.0, // AI-refined content has high confidence
      speaker: speakerPrefix,
      isFinal: true,
      isManuallyEdited: false,
      isAIRefined: true // Mark as AI-refined
    }));
  }

  /**
   * Transcribe audio file using Gemini Multimodal API
   * Gemini API officially supports: WAV and MP3 only
   * Other formats (WebM, MP4, OGG, AAC, FLAC) must be converted to WAV first
   */
  public static async transcribeAudioWithGemini(
    apiKey: string,
    audioBlob: Blob,
    modelName: string, // e.g., "models/gemini-1.5-flash" or "models/gemini-2.0-flash-exp"
    onProgress?: (progress: number) => void,
    skipSizeCheck: boolean = false, // Skip size check when called from auto-split flow
    maxFileSizeMB: number = 20 // Maximum file size in MB (from config)
  ): Promise<TranscriptionResult[]> {
    if (!apiKey || apiKey.trim().length === 0) {
      throw new Error('Gemini API Key is required');
    }

    if (!modelName || !modelName.startsWith('models/')) {
      throw new Error('Please select a Gemini model in Settings');
    }

    // Validate file size (limit from config)
    // Skip this check when called from transcribeEntireAudioWithGemini (already split into valid chunks)
    if (!skipSizeCheck) {
      const MAX_FILE_SIZE = maxFileSizeMB * 1024 * 1024;
      const fileSizeMB = audioBlob.size / (1024 * 1024);
      
      console.log(`üìä File size: ${fileSizeMB.toFixed(2)} MB (Limit: ${maxFileSizeMB} MB)`);
      
      if (audioBlob.size > MAX_FILE_SIZE) {
        // Return special error object with file size info
        const error: any = new Error('FILE_TOO_LARGE');
        error.fileSizeMB = fileSizeMB;
        error.maxSizeMB = maxFileSizeMB;
        throw error;
      }
    }

    if (onProgress) onProgress(10);

    try {
      // Convert to WAV if needed
      // Gemini API officially supports: WAV and MP3 only
      // All other formats need conversion to WAV
      let processedAudio = audioBlob;
      const audioType = audioBlob.type.toLowerCase();
      const isWavOrMp3 = audioType.includes('wav') || audioType.includes('mpeg') || audioType.includes('mp3');
      const needsConversion = !isWavOrMp3;
      
      if (needsConversion) {
        console.log(`üîÑ Converting ${audioType} to WAV (Gemini requires WAV or MP3)...`);
        if (onProgress) onProgress(15);
        
        const originalSizeMB = audioBlob.size / (1024 * 1024);
        
        // Convert with lower sample rate if file is large
        const targetSampleRate = audioBlob.size > 10 * 1024 * 1024 ? 16000 : 44100;
        processedAudio = await this.convertToWav(audioBlob, targetSampleRate);
        
        const newSizeMB = processedAudio.size / (1024 * 1024);
        console.log(`‚úÖ Converted: ${originalSizeMB.toFixed(2)}MB ‚Üí ${newSizeMB.toFixed(2)}MB`);
        
        // Check again after conversion (only if not skipping size check)
        if (!skipSizeCheck) {
          const MAX_FILE_SIZE = maxFileSizeMB * 1024 * 1024;
          
          if (processedAudio.size > MAX_FILE_SIZE) {
            throw new Error(
              `‚ùå Sau chuy·ªÉn ƒë·ªïi, file v·∫´n qu√° l·ªõn: ${newSizeMB.toFixed(2)} MB\n\n` +
              `Vui l√≤ng gi·∫£m th·ªùi l∆∞·ª£ng ghi √¢m ho·∫∑c gi·∫£m ch·∫•t l∆∞·ª£ng.`
            );
          }
        }
        
        if (onProgress) onProgress(25);
      }

      // Convert audio blob to base64
      const base64Audio = await this.blobToBase64(processedAudio);
      if (onProgress) onProgress(30);

      // Get MIME type (use WAV if converted)
      const mimeType = processedAudio.type || 'audio/wav';

      // Prepare request
      const endpoint = `https://generativelanguage.googleapis.com/${this.GEMINI_API_VERSION}/${modelName}:generateContent?key=${apiKey}`;

      const requestBody = {
        contents: [{
          parts: [
            {
              text: `H√£y nghe file √¢m thanh cu·ªôc h·ªçp n√†y v√† chuy·ªÉn th√†nh vƒÉn b·∫£n. Y√™u c·∫ßu b·∫Øt bu·ªôc:

1. Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n h·ªôi tho·∫°i t·ª± nhi√™n.
2. G·∫Øn nh√£n th·ªùi gian [mm:ss] v√†o ƒë·∫ßu m·ªói ƒëo·∫°n d·ª±a tr√™n th·ªùi ƒëi·ªÉm ng∆∞·ªùi n√≥i b·∫Øt ƒë·∫ßu trong file √¢m thanh.
3. N·∫øu c√≥ nhi·ªÅu ng∆∞·ªùi n√≥i, h√£y ph√¢n bi·ªát b·∫±ng c√°ch ghi 'Ng∆∞·ªùi n√≥i 1:', 'Ng∆∞·ªùi n√≥i 2:'...
4. L√†m s·∫°ch vƒÉn b·∫£n (lo·∫°i b·ªè t·ª´ ƒë·ªám, s·ª≠a l·ªói ch√≠nh t·∫£).
5. ƒê·ªãnh d·∫°ng: Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON v·ªõi c·∫•u tr√∫c:
{
  "segments": [
    {
      "timestamp": "mm:ss",
      "speaker": "Ng∆∞·ªùi n√≥i 1",
      "text": "n·ªôi dung"
    }
  ]
}`
            },
            {
              inline_data: {
                mime_type: mimeType,
                data: base64Audio
              }
            }
          ]
        }]
      };

      if (onProgress) onProgress(40);

      // Make API request
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody)
      });

      if (onProgress) onProgress(70);

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(
          `Gemini API error (${response.status}): ${errorData.error?.message || response.statusText}`
        );
      }

      const data = await response.json();
      if (onProgress) onProgress(90);

      // Parse response
      const results = this.parseGeminiAudioTranscription(data);
      if (onProgress) onProgress(100);

      return results;

    } catch (error: any) {
      console.error('Gemini audio transcription error:', error);
      throw new Error(`Failed to transcribe audio: ${error.message}`);
    }
  }

  /**
   * Convert Blob to Base64 string
   */
  private static blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        const base64 = (reader.result as string).split(',')[1]; // Remove data:audio/...;base64, prefix
        resolve(base64);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  /**
   * Convert audio blob to WAV format with optional sample rate optimization
   * Gemini API officially supports WAV and MP3 only
   * @param targetSampleRate - Target sample rate (16000 for smaller files, 44100 for quality)
   */
  private static async convertToWav(audioBlob: Blob, targetSampleRate: number = 44100): Promise<Blob> {
    return new Promise((resolve, reject) => {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const reader = new FileReader();

      reader.onload = async (e) => {
        try {
          const arrayBuffer = e.target?.result as ArrayBuffer;
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

          // Resample if needed to reduce file size
          let finalBuffer = audioBuffer;
          if (audioBuffer.sampleRate !== targetSampleRate) {
            console.log(`üîä Resampling: ${audioBuffer.sampleRate}Hz ‚Üí ${targetSampleRate}Hz`);
            finalBuffer = await this.resampleAudioBuffer(audioBuffer, targetSampleRate);
          }

          // Convert to WAV
          const wavBlob = this.audioBufferToWav(finalBuffer);
          resolve(wavBlob);
        } catch (error) {
          reject(error);
        }
      };

      reader.onerror = reject;
      reader.readAsArrayBuffer(audioBlob);
    });
  }

  /**
   * Resample AudioBuffer to target sample rate (reduces file size)
   */
  private static async resampleAudioBuffer(audioBuffer: AudioBuffer, targetSampleRate: number): Promise<AudioBuffer> {
    const offlineContext = new OfflineAudioContext(
      audioBuffer.numberOfChannels,
      audioBuffer.duration * targetSampleRate,
      targetSampleRate
    );

    const source = offlineContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(offlineContext.destination);
    source.start();

    return await offlineContext.startRendering();
  }

  /**
   * Convert AudioBuffer to WAV Blob
   */
  private static audioBufferToWav(audioBuffer: AudioBuffer): Blob {
    const numberOfChannels = audioBuffer.numberOfChannels;
    const sampleRate = audioBuffer.sampleRate;
    const format = 1; // PCM
    const bitDepth = 16;

    const bytesPerSample = bitDepth / 8;
    const blockAlign = numberOfChannels * bytesPerSample;

    const data = [];
    for (let i = 0; i < audioBuffer.numberOfChannels; i++) {
      data.push(audioBuffer.getChannelData(i));
    }

    const interleaved = this.interleave(data);
    const dataLength = interleaved.length * bytesPerSample;
    const buffer = new ArrayBuffer(44 + dataLength);
    const view = new DataView(buffer);

    // Write WAV header
    this.writeString(view, 0, 'RIFF');
    view.setUint32(4, 36 + dataLength, true);
    this.writeString(view, 8, 'WAVE');
    this.writeString(view, 12, 'fmt ');
    view.setUint32(16, 16, true); // fmt chunk size
    view.setUint16(20, format, true);
    view.setUint16(22, numberOfChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * blockAlign, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bitDepth, true);
    this.writeString(view, 36, 'data');
    view.setUint32(40, dataLength, true);

    // Write audio data
    this.floatTo16BitPCM(view, 44, interleaved);

    return new Blob([buffer], { type: 'audio/wav' });
  }

  /**
   * Interleave multiple audio channels
   */
  private static interleave(channelData: Float32Array[]): Float32Array {
    const length = channelData[0].length;
    const numberOfChannels = channelData.length;
    const result = new Float32Array(length * numberOfChannels);

    let offset = 0;
    for (let i = 0; i < length; i++) {
      for (let channel = 0; channel < numberOfChannels; channel++) {
        result[offset++] = channelData[channel][i];
      }
    }

    return result;
  }
  /**
   * Split audio into chunks based on size AND duration limits
   * Each chunk must satisfy: size <= maxChunkSizeMB AND duration <= maxDurationMinutes
   * @param audioBlob - Audio blob to split (should be WAV format)
   * @param maxChunkSizeMB - Maximum size per chunk in MB (default: 20)
   * @param maxDurationMinutes - Maximum duration per chunk in minutes (default: 60)
   * @returns Array of chunks with blob, startTimeMs, endTimeMs
   */
  public static async splitAudioIntoChunks(
    audioBlob: Blob,
    maxChunkSizeMB: number = 20,
    maxDurationMinutes: number = 60
  ): Promise<{ blob: Blob; startTimeMs: number; endTimeMs: number }[]> {
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
    const reader = new FileReader();

    return new Promise((resolve, reject) => {
      reader.onload = async (e) => {
        try {
          const arrayBuffer = e.target?.result as ArrayBuffer;
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

          const totalDurationMs = audioBuffer.duration * 1000;
          const totalDurationMinutes = totalDurationMs / (60 * 1000);
          const totalSizeMB = audioBlob.size / (1024 * 1024);

          // Calculate number of chunks needed based on BOTH constraints
          // 1. Chunks needed based on size
          const chunksBySizeCount = Math.ceil(totalSizeMB / maxChunkSizeMB);
          
          // 2. Chunks needed based on duration
          const chunksByDurationCount = Math.ceil(totalDurationMinutes / maxDurationMinutes);
          
          // Take the MAXIMUM to satisfy BOTH constraints
          const numberOfChunks = Math.max(chunksBySizeCount, chunksByDurationCount);
          const chunkDurationMs = totalDurationMs / numberOfChunks;
          const chunkDurationMinutes = chunkDurationMs / (60 * 1000);

          console.log(`üìè Audio info: ${totalSizeMB.toFixed(2)}MB, ${totalDurationMinutes.toFixed(1)} minutes`);
          console.log(`üìä Constraints: maxSize=${maxChunkSizeMB}MB, maxDuration=${maxDurationMinutes} minutes`);
          console.log(`üì¶ Splitting into ${numberOfChunks} chunks (by size: ${chunksBySizeCount}, by duration: ${chunksByDurationCount})`);
          console.log(`‚è±Ô∏è Each chunk: ~${chunkDurationMinutes.toFixed(1)} minutes, ~${(totalSizeMB / numberOfChunks).toFixed(2)}MB`);

          const chunks: { blob: Blob; startTimeMs: number; endTimeMs: number }[] = [];

          for (let i = 0; i < numberOfChunks; i++) {
            const startTimeMs = i * chunkDurationMs;
            const endTimeMs = Math.min((i + 1) * chunkDurationMs, totalDurationMs);

            console.log(`‚è±Ô∏è Extracting chunk ${i + 1}/${numberOfChunks}: ${startTimeMs.toFixed(0)}ms - ${endTimeMs.toFixed(0)}ms`);

            const chunkBlob = await this.extractAudioSegment(audioBlob, startTimeMs, endTimeMs);

            chunks.push({
              blob: chunkBlob,
              startTimeMs,
              endTimeMs
            });
          }

          resolve(chunks);
        } catch (error) {
          reject(error);
        }
      };

      reader.onerror = reject;
      reader.readAsArrayBuffer(audioBlob);
    });
  }

  /**
   * Extract a segment from audio blob based on time range
   * @param audioBlob - Original audio blob
   * @param startTimeMs - Start time in milliseconds
   * @param endTimeMs - End time in milliseconds
   * @returns Promise<Blob> - Audio segment blob
   */
  public static async extractAudioSegment(
    audioBlob: Blob,
    startTimeMs: number,
    endTimeMs: number
  ): Promise<Blob> {
    return new Promise((resolve, reject) => {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const reader = new FileReader();

      reader.onload = async (e) => {
        try {
          const arrayBuffer = e.target?.result as ArrayBuffer;
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

          // Calculate start and end in samples
          const startSample = Math.floor((startTimeMs / 1000) * audioBuffer.sampleRate);
          const endSample = Math.floor((endTimeMs / 1000) * audioBuffer.sampleRate);
          const segmentLength = endSample - startSample;

          // Create new buffer for the segment
          const segmentBuffer = audioContext.createBuffer(
            audioBuffer.numberOfChannels,
            segmentLength,
            audioBuffer.sampleRate
          );

          // Copy data for each channel
          for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
            const channelData = audioBuffer.getChannelData(channel);
            const segmentData = segmentBuffer.getChannelData(channel);
            for (let i = 0; i < segmentLength; i++) {
              segmentData[i] = channelData[startSample + i];
            }
          }

          // Convert to WAV
          const wavBlob = this.audioBufferToWav(segmentBuffer);
          resolve(wavBlob);
        } catch (error) {
          reject(error);
        }
      };

      reader.onerror = reject;
      reader.readAsArrayBuffer(audioBlob);
    });
  }

  /**
   * Process entire audio file by automatically splitting into chunks
   * Respects Gemini API limits: 15 req/min, 1500 req/day, configurable MB per file and duration
   * @param apiKey - Gemini API key
   * @param audioBlob - Original audio blob (can be > configurable limit)
   * @param modelName - Gemini model name
   * @param onProgress - Progress callback (progress: number, message: string)
   * @param maxFileSizeMB - Maximum file size in MB (from config, default: 20)
   * @param requestDelaySeconds - Delay between requests in seconds (from config, default: 5)
   * @param maxDurationMinutes - Maximum duration per chunk in minutes (from config, default: 60)
   * @returns Promise<TranscriptionResult[]> - All transcription results, sorted by timestamp
   */
  public static async transcribeEntireAudioWithGemini(
    apiKey: string,
    audioBlob: Blob,
    modelName: string,
    onProgress?: (progress: number, message: string) => void,
    maxFileSizeMB: number = 20,
    requestDelaySeconds: number = 5,
    maxDurationMinutes: number = 60
  ): Promise<TranscriptionResult[]> {
    const maxSizeMB = maxFileSizeMB;

    // CRITICAL: Convert to WAV first if needed, THEN split based on size
    // Gemini API officially supports: WAV and MP3 only
    // All other formats (WebM, MP4, OGG, AAC, FLAC) must be converted to WAV
    if (onProgress) onProgress(3, 'ƒêang ki·ªÉm tra ƒë·ªãnh d·∫°ng audio...');
    
    let wavBlob = audioBlob;
    const audioType = audioBlob.type.toLowerCase();
    const isWavOrMp3 = audioType.includes('wav') || audioType.includes('mpeg') || audioType.includes('mp3');
    const needsConversion = !isWavOrMp3;
    
    if (needsConversion) {
      if (onProgress) onProgress(5, `ƒêang chuy·ªÉn ƒë·ªïi ${audioType} sang WAV...`);
      // Convert with lower sample rate for smaller file size
      const targetSampleRate = 16000; // Lower sample rate = smaller file
      wavBlob = await this.convertToWav(audioBlob, targetSampleRate);
      
      const originalSizeMB = audioBlob.size / (1024 * 1024);
      const wavSizeMB = wavBlob.size / (1024 * 1024);
      console.log(`‚úÖ Converted ${audioType}: ${originalSizeMB.toFixed(2)}MB ‚Üí ${wavSizeMB.toFixed(2)}MB (WAV)`);
    } else {
      console.log(`‚úÖ Audio format ${audioType} is supported by Gemini (WAV/MP3) - no conversion needed`);
    }

    // Now split the WAV file into chunks based on actual WAV size AND duration
    if (onProgress) onProgress(8, 'ƒêang ph√¢n t√≠ch v√† chia file WAV...');
    const chunks = await this.splitAudioIntoChunks(wavBlob, maxSizeMB, maxDurationMinutes);

    if (onProgress) onProgress(10, `ƒê√£ chia th√†nh ${chunks.length} ph·∫ßn. B·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi...`);

    const allResults: TranscriptionResult[] = [];

    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      const chunkProgress = 10 + ((i / chunks.length) * 80);

      if (onProgress) {
        onProgress(
          chunkProgress,
          `ƒêang x·ª≠ l√Ω ph·∫ßn ${i + 1}/${chunks.length}...`
        );
      }

      try {
        // Transcribe this chunk (skip size check - already validated and split)
        const results = await this.transcribeAudioWithGemini(
          apiKey,
          chunk.blob,
          modelName,
          (subProgress) => {
            if (onProgress) {
              const totalProgress = chunkProgress + (subProgress / chunks.length) * 0.8;
              onProgress(totalProgress, `Ph·∫ßn ${i + 1}/${chunks.length}: ${subProgress.toFixed(0)}%`);
            }
          },
          true, // skipSizeCheck = true (chunks already validated)
          maxSizeMB // Pass maxFileSizeMB to child call
        );

        // Adjust timestamps for this chunk
        const adjustedResults = this.adjustTimestamps(results, chunk.startTimeMs);
        allResults.push(...adjustedResults);

        // Add delay between chunks to respect rate limits (15 req/min)
        if (i < chunks.length - 1) {
          if (onProgress) {
            onProgress(
              chunkProgress + 5,
              `ƒê·ª£i ${requestDelaySeconds}s tr∆∞·ªõc khi x·ª≠ l√Ω ph·∫ßn ti·∫øp theo...`
            );
          }
          await new Promise(resolve => setTimeout(resolve, requestDelaySeconds * 1000));
        }
      } catch (error: any) {
        // Handle quota errors
        if (error.message.includes('429') || error.message.includes('quota')) {
          throw new Error(
            `V∆∞·ª£t h·∫°n m·ª©c API t·∫°i ph·∫ßn ${i + 1}/${chunks.length}.\n\n` +
            `‚úÖ ƒê√£ x·ª≠ l√Ω: ${i}/${chunks.length} ph·∫ßn\n` +
            `‚ùå L·ªói: ${error.message}\n\n` +
            `üí° ƒê·ª£i 24 gi·ªù ho·∫∑c n√¢ng c·∫•p Paid tier.`
          );
        }
        throw error;
      }
    }

    // Sort by timestamp
    allResults.sort((a, b) => (a.audioTimeMs || 0) - (b.audioTimeMs || 0));

    if (onProgress) onProgress(100, `Ho√†n th√†nh! ${allResults.length} segments`);

    console.log(`‚úÖ Transcribed entire audio: ${allResults.length} segments from ${chunks.length} chunks`);
    return allResults;
  }

  /**
   * Adjust timestamps in transcription results based on segment start time
   * @param results - Transcription results from segment
   * @param offsetMs - Offset in milliseconds (segment start time)
   * @returns Adjusted transcription results
   */
  public static adjustTimestamps(
    results: TranscriptionResult[],
    offsetMs: number
  ): TranscriptionResult[] {
    return results.map(result => ({
      ...result,
      audioTimeMs: result.audioTimeMs ? result.audioTimeMs + offsetMs : undefined
    }));
  }

  /*
   * FUTURE ENHANCEMENT: optimizeAudioFormat() method
   * Could convert WAV to WebM/Opus to reduce file size by 70-90%
   * Reserved for future implementation
   */
  /**
   * Write string to DataView
   */
  private static writeString(view: DataView, offset: number, string: string): void {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i));
    }
  }

  /**
   * Convert Float32 samples to 16-bit PCM
   */
  private static floatTo16BitPCM(view: DataView, offset: number, input: Float32Array): void {
    for (let i = 0; i < input.length; i++, offset += 2) {
      const s = Math.max(-1, Math.min(1, input[i]));
      view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
    }
  }

  /**
   * Parse Gemini audio transcription response
   */
  private static parseGeminiAudioTranscription(apiResponse: any): TranscriptionResult[] {
    try {
      const candidates = apiResponse.candidates;
      if (!candidates || candidates.length === 0) {
        throw new Error('No response from Gemini API');
      }

      const content = candidates[0].content;
      if (!content || !content.parts || content.parts.length === 0) {
        throw new Error('Empty response from Gemini');
      }

      const textResponse = content.parts[0].text;
      if (!textResponse) {
        throw new Error('No text in Gemini response');
      }

      // Extract JSON from response (handle markdown code blocks)
      let jsonText = textResponse.trim();
      const jsonMatch = jsonText.match(/```json\s*([\s\S]*?)```/) || jsonText.match(/```\s*([\s\S]*?)```/);
      if (jsonMatch) {
        jsonText = jsonMatch[1].trim();
      }

      const parsed = JSON.parse(jsonText);

      if (!parsed.segments || !Array.isArray(parsed.segments)) {
        throw new Error('Invalid JSON structure: missing segments array');
      }

      // Convert to TranscriptionResult format
      return parsed.segments.map((segment: any, index: number) => {
        const timestamp = segment.timestamp || '0:00';
        const audioTimeMs = this.parseTimestampToMs(timestamp);

        // For Gemini transcription, we don't have actual wall-clock time
        // Use current time as base, but mark it clearly
        const now = new Date();
        
        return {
          id: `gemini-${Date.now()}-${index}`,
          text: segment.text || '',
          startTime: now.toISOString(), // Use ISO format to avoid NaN display
          endTime: now.toISOString(),   // Same time since we don't have duration
          audioTimeMs, // This is the relative position in audio file (mm:ss)
          confidence: 1.0,
          speaker: segment.speaker || 'Unknown',
          isFinal: true,
          isManuallyEdited: false,
          isAIRefined: true
        };
      });

    } catch (error: any) {
      console.error('Failed to parse Gemini audio transcription:', error);
      console.log('Raw API response:', JSON.stringify(apiResponse, null, 2));
      throw new Error(`Failed to parse Gemini response: ${error.message}`);
    }
  }

  /**
   * Parse timestamp string (mm:ss or m:ss) to milliseconds
   */
  private static parseTimestampToMs(timestamp: string): number {
    try {
      const parts = timestamp.split(':').map(p => parseInt(p.trim(), 10));
      if (parts.length === 2) {
        const [minutes, seconds] = parts;
        return (minutes * 60 + seconds) * 1000;
      }
      return 0;
    } catch {
      return 0;
    }
  }
}
