import type { TranscriptionResult } from '../types/types';

export interface RawTranscriptData {
  text: string;
  timestamp: string;
  audioTimeMs?: number;
  confidence: number;
  isFinal: boolean;
}

export interface RefinedSegment {
  text: string;
  timestamp: string;
  audioTimeMs?: number;
}

/**
 * AI Refinement Service for Gemini AI
 * Refines raw speech-to-text transcripts with AI
 */
export class AIRefinementService {
  private static readonly GEMINI_MODELS_ENDPOINT = 'https://generativelanguage.googleapis.com/v1beta/models';
  private static readonly GEMINI_API_VERSION = 'v1beta'; // Use v1beta as it's more stable

  // Gemini Free Tier Limits (per day)
  private static readonly FREE_TIER_LIMITS = {
    RPM: 15,           // Requests per minute
    TPM: 1000000,      // Tokens per minute (1M)
    RPD: 1500,         // Requests per day
    TPD: 250000        // Tokens per day (250K) - Main limit users hit
  };

  // Estimate tokens per segment (rough estimation)
  private static readonly BATCH_SIZE = 50; // Process 50 segments at a time (~7500 tokens)

  /**
   * Estimate token count for transcripts
   */
  private static estimateTokenCount(transcriptions: TranscriptionResult[]): number {
    // Rough estimation: 1 token ‚âà 4 characters for English, ~2-3 for Vietnamese
    const totalChars = transcriptions.reduce((sum, t) => sum + t.text.length, 0);
    // Vietnamese: ~2.5 chars per token, English: ~4 chars per token
    // Use 3 as average + overhead for prompt
    const estimatedTokens = Math.ceil(totalChars / 3) + 1000; // +1000 for prompt overhead
    return estimatedTokens;
  }

  /**
   * Check if processing would exceed quota
   */
  private static checkQuotaEstimate(transcriptions: TranscriptionResult[]): {
    estimatedTokens: number;
    withinLimit: boolean;
    message: string;
  } {
    const estimatedTokens = this.estimateTokenCount(transcriptions);
    const withinLimit = estimatedTokens < this.FREE_TIER_LIMITS.TPD;

    let message = '';
    if (!withinLimit) {
      message = `‚ö†Ô∏è ∆Ø·ªõc t√≠nh ${estimatedTokens.toLocaleString()} tokens - v∆∞·ª£t h·∫°n m·ª©c mi·ªÖn ph√≠ (${this.FREE_TIER_LIMITS.TPD.toLocaleString()} tokens/ng√†y)`;
    } else {
      const percentUsed = Math.round((estimatedTokens / this.FREE_TIER_LIMITS.TPD) * 100);
      message = `‚úÖ ∆Ø·ªõc t√≠nh ${estimatedTokens.toLocaleString()} tokens (~${percentUsed}% h·∫°n m·ª©c mi·ªÖn ph√≠)`;
    }

    return { estimatedTokens, withinLimit, message };
  }

  /**
   * Split transcriptions into batches for processing
   */
  private static splitIntoBatches(transcriptions: TranscriptionResult[], batchSize: number): TranscriptionResult[][] {
    const batches: TranscriptionResult[][] = [];
    for (let i = 0; i < transcriptions.length; i += batchSize) {
      batches.push(transcriptions.slice(i, i + batchSize));
    }
    return batches;
  }

  /**
   * Check quota status by making a minimal test request
   * Returns usage info and recommendations
   */
  public static async checkQuotaStatus(apiKey: string, modelName: string): Promise<{
    status: 'available' | 'limited' | 'exceeded' | 'error';
    message: string;
    recommendations: string[];
  }> {
    try {
      const endpoint = `https://generativelanguage.googleapis.com/${this.GEMINI_API_VERSION}/${modelName}:generateContent`;
      
      const response = await fetch(`${endpoint}?key=${apiKey}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ parts: [{ text: 'test' }] }],
          generationConfig: { maxOutputTokens: 1 }
        })
      });
      
      if (response.status === 429) {
        const errorData = await response.json().catch(() => ({}));
        const errorMsg = errorData.error?.message || '';
        
        if (errorMsg.includes('quota') || errorMsg.includes('250000')) {
          return {
            status: 'exceeded',
            message: 'üö´ ƒê√£ v∆∞·ª£t h·∫°n m·ª©c 250,000 tokens/ng√†y',
            recommendations: [
              'üïí ƒê·ª£i 24 gi·ªù ƒë·ªÉ quota reset',
              'üí≥ N√¢ng c·∫•p Paid tier: ~$2/th√°ng, unlimited',
              'üìä Monitor: https://ai.dev/rate-limit'
            ]
          };
        } else {
          return {
            status: 'limited',
            message: '‚è±Ô∏è V∆∞·ª£t 15 requests/ph√∫t',
            recommendations: [
              '‚è∞ ƒê·ª£i 1-2 ph√∫t r·ªìi th·ª≠ l·∫°i',
              'üîÑ App s·∫Ω t·ª± ƒë·ªông delay gi·ªØa c√°c batch'
            ]
          };
        }
      } else if (response.ok) {
        return {
          status: 'available',
          message: '‚úÖ API Key ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng',
          recommendations: [
            'üéØ Free tier: 250,000 tokens/ng√†y',
            'üìä M·ªói 50 segments ~ 7,500 tokens',
            'üîç Monitor: https://ai.dev/rate-limit'
          ]
        };
      } else {
        return {
          status: 'error',
          message: `‚ùå L·ªói API: ${response.status}`,
          recommendations: [
            'Ki·ªÉm tra API Key c√≥ h·ª£p l·ªá',
            'Ki·ªÉm tra model ƒë√£ ch·ªçn ƒë√∫ng'
          ]
        };
      }
    } catch (error: any) {
      return {
        status: 'error',
        message: 'Kh√¥ng th·ªÉ ki·ªÉm tra quota',
        recommendations: [
          'Ki·ªÉm tra k·∫øt n·ªëi internet',
          'Th·ª≠ l·∫°i sau v√†i ph√∫t'
        ]
      };
    }
  }

  /**
   * Get usage metadata and quota information from Gemini API
   * Note: Gemini API doesn't provide direct quota endpoint, but we can infer from rate limit headers
   */
  public static async checkQuotaInfo(apiKey: string, modelName: string): Promise<{
    estimatedUsage: string;
    quotaStatus: string;
    recommendations: string[];
  }> {
    try {
      // Make a minimal test request to check quota status
      const endpoint = `https://generativelanguage.googleapis.com/${this.GEMINI_API_VERSION}/${modelName}:generateContent`;
      
      const response = await fetch(`${endpoint}?key=${apiKey}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          contents: [{
            parts: [{ text: 'test' }]
          }],
          generationConfig: {
            maxOutputTokens: 1
          }
        })
      });

      // Check response headers for quota info (if available)
      // const remainingRequests = response.headers.get('x-ratelimit-remaining');
      // const resetTime = response.headers.get('x-ratelimit-reset');
      
      // Parse response to check for quota errors
      const recommendations: string[] = [];
      let quotaStatus = 'unknown';
      let estimatedUsage = 'Kh√¥ng c√≥ th√¥ng tin chi ti·∫øt';

      if (response.status === 429) {
        quotaStatus = 'exceeded';
        const errorData = await response.json().catch(() => ({}));
        const errorMsg = errorData.error?.message || '';
        
        if (errorMsg.includes('quota')) {
          estimatedUsage = 'ƒê√£ v∆∞·ª£t h·∫°n m·ª©c 250,000 tokens/ng√†y';
          recommendations.push('ƒê·ª£i 24 gi·ªù ƒë·ªÉ quota reset');
          recommendations.push('Ho·∫∑c n√¢ng c·∫•p l√™n Paid tier (~$2/th√°ng)');
        } else {
          estimatedUsage = 'ƒê√£ v∆∞·ª£t 15 requests/ph√∫t';
          recommendations.push('ƒê·ª£i 1 ph√∫t r·ªìi th·ª≠ l·∫°i');
        }
      } else if (response.ok) {
        quotaStatus = 'available';
        estimatedUsage = 'API Key ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng';
        
        // Estimate based on typical usage
        recommendations.push('‚úÖ Free tier: 250,000 tokens/ng√†y, 15 requests/ph√∫t');
        recommendations.push('üí° M·ªói 50 segments ~ 7,500 tokens');
        recommendations.push('üìä Monitor: https://ai.dev/rate-limit');
      }

      return {
        estimatedUsage,
        quotaStatus,
        recommendations
      };
    } catch (error: any) {
      return {
        estimatedUsage: 'Kh√¥ng th·ªÉ ki·ªÉm tra quota',
        quotaStatus: 'error',
        recommendations: [
          'Ki·ªÉm tra API Key c√≥ h·ª£p l·ªá',
          'Ki·ªÉm tra k·∫øt n·ªëi internet',
          'Th·ª≠ l·∫°i sau v√†i ph√∫t'
        ]
      };
    }
  }

  /**
   * List available Gemini models for the given API key
   * Useful for debugging and verifying API key access
   */
  public static async listGeminiModels(apiKey: string): Promise<any> {
    if (!apiKey || apiKey.trim().length === 0) {
      throw new Error('API Key is required');
    }

    try {
      const response = await fetch(`${this.GEMINI_MODELS_ENDPOINT}?key=${apiKey}`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        }
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(
          `Failed to list models (${response.status}): ${errorData.error?.message || response.statusText}\n` +
          `URL: ${this.GEMINI_MODELS_ENDPOINT}`
        );
      }

      const data = await response.json();
      return data;
    } catch (error: any) {
      console.error('Error listing Gemini models:', error);
      throw new Error(`Cannot list Gemini models: ${error.message}`);
    }
  }

  /**
   * Refine transcripts using Gemini AI with automatic batching
   * @param transcriptions - Primary data (user-edited, highest reliability)
   * @param rawData - Supplementary data (original Web Speech API output for reference)
   */
  public static async refineTranscripts(
    apiKey: string,
    transcriptions: TranscriptionResult[], // Primary data source
    rawData: RawTranscriptData[], // Optional: supplementary raw data
    modelName: string, // REQUIRED: specific Gemini model (e.g., "models/gemini-2.5-flash")
    onProgress?: (progress: number) => void
  ): Promise<RefinedSegment[]> {
    // Check quota estimate first
    const quotaCheck = this.checkQuotaEstimate(transcriptions);
    console.log('üìä Quota Check:', quotaCheck.message);

    // If estimated tokens exceed limit, use batch processing
    if (quotaCheck.estimatedTokens > this.FREE_TIER_LIMITS.TPD * 0.8) { // 80% threshold
      console.log('üîÑ Using batch processing to avoid quota limits...');
      return this.refineTranscriptsInBatches(apiKey, transcriptions, rawData, modelName, onProgress);
    }

    // Otherwise, process normally
    return this.refineWithGemini(apiKey, transcriptions, rawData, modelName, onProgress);
  }

  /**
   * Refine transcripts in batches to avoid quota limits
   */
  private static async refineTranscriptsInBatches(
    apiKey: string,
    transcriptions: TranscriptionResult[],
    rawData: RawTranscriptData[],
    modelName: string,
    onProgress?: (progress: number) => void
  ): Promise<RefinedSegment[]> {
    const batches = this.splitIntoBatches(transcriptions, this.BATCH_SIZE);
    const allRefinedSegments: RefinedSegment[] = [];

    console.log(`üì¶ Processing ${transcriptions.length} segments in ${batches.length} batches...`);

    for (let i = 0; i < batches.length; i++) {
      const batch = batches[i];
      const batchProgress = (i / batches.length) * 100;

      console.log(`üîÑ Processing batch ${i + 1}/${batches.length} (${batch.length} segments)...`);

      try {
        // Find corresponding raw data for this batch
        const batchStartIndex = i * this.BATCH_SIZE;
        const batchRawData = rawData.slice(batchStartIndex, batchStartIndex + batch.length);

        // Process this batch
        const refinedBatch = await this.refineWithGemini(
          apiKey,
          batch,
          batchRawData,
          modelName,
          (subProgress) => {
            if (onProgress) {
              const totalProgress = batchProgress + (subProgress / batches.length);
              onProgress(Math.min(totalProgress, 99));
            }
          }
        );

        allRefinedSegments.push(...refinedBatch);

        // Add delay between batches to avoid rate limiting (except for last batch)
        if (i < batches.length - 1) {
          console.log('‚è≥ Waiting 5 seconds before next batch...');
          await new Promise(resolve => setTimeout(resolve, 5000));
        }
      } catch (error: any) {
        // If quota exceeded, throw error with helpful message
        if (error.message.includes('429') || error.message.includes('quota')) {
          throw new Error(
            `V∆∞·ª£t h·∫°n m·ª©c API t·∫°i batch ${i + 1}/${batches.length}.\n\n` +
            `‚úÖ ƒê√£ x·ª≠ l√Ω: ${allRefinedSegments.length}/${transcriptions.length} segments\n\n` +
            `Nguy√™n nh√¢n: ${error.message}\n\n` +
            `üí° Gi·∫£i ph√°p:\n` +
            `‚Ä¢ ƒê·ª£i 24 gi·ªù ƒë·ªÉ quota reset (h·∫°n m·ª©c: 250,000 tokens/ng√†y)\n` +
            `‚Ä¢ Ho·∫∑c n√¢ng c·∫•p l√™n Gemini API tr·∫£ ph√≠ t·∫°i console.cloud.google.com`
          );
        }
        throw error;
      }
    }

    if (onProgress) onProgress(100);
    console.log(`‚úÖ Batch processing complete: ${allRefinedSegments.length} segments refined`);
    return allRefinedSegments;
  }

  /**
   * Refine with Google Gemini API using user-selected model
   */
  private static async refineWithGemini(
    apiKey: string,
    transcriptions: TranscriptionResult[], // Primary data
    rawData: RawTranscriptData[], // Supplementary data
    modelName: string, // REQUIRED: specific model like "models/gemini-2.5-flash"
    onProgress?: (progress: number) => void
  ): Promise<RefinedSegment[]> {
    if (!apiKey || apiKey.trim().length === 0) {
      throw new Error('API Key is required for AI refinement');
    }

    if (transcriptions.length === 0) {
      throw new Error('No transcript data to refine');
    }

    // Validate model name
    if (!modelName || !modelName.trim() || !modelName.startsWith('models/')) {
      throw new Error(
        'Vui l√≤ng ch·ªçn Gemini Model trong Settings.\n\n' +
        'B∆∞·ªõc 1: M·ªü Settings ‚Üí Nh·∫≠p Gemini API Key\n' +
        'B∆∞·ªõc 2: Ch·ªù h·ªá th·ªëng t·∫£i danh s√°ch models\n' +
        'B∆∞·ªõc 3: Ch·ªçn model t·ª´ dropdown (v√≠ d·ª•: Gemini 2.5 Flash)\n' +
        'B∆∞·ªõc 4: L∆∞u v√† th·ª≠ l·∫°i'
      );
    }

    try {
      // Prepare primary data from transcriptions (user-edited, highest reliability)
      const transcriptData = transcriptions.map((item, index) => ({
        index: index + 1,
        timestamp: item.startTime,
        audioTime: item.audioTimeMs !== undefined ? this.formatAudioTime(item.audioTimeMs) : undefined,
        text: item.text,
        confidence: item.confidence,
        type: item.isFinal ? 'final' : 'interim'
      }));

      // Prepare supplementary raw data (if available)
      const hasRawData = rawData && rawData.length > 0;
      const rawMetadata = hasRawData ? rawData.map((item, index) => ({
        index: index + 1,
        timestamp: item.timestamp,
        audioTime: item.audioTimeMs !== undefined ? this.formatAudioTime(item.audioTimeMs) : undefined,
        text: item.text,
        confidence: item.confidence,
        type: item.isFinal ? 'final' : 'interim'
      })) : null;

      // Create prompt
      const prompt = this.createRefinementPrompt(transcriptData, rawMetadata);

      if (onProgress) onProgress(10);

      // Build endpoint URL with selected model
      const endpoint = `https://generativelanguage.googleapis.com/${this.GEMINI_API_VERSION}/${modelName}:generateContent`;
      console.log(`ü§ñ Using Gemini model: ${modelName}`);
      console.log(`üì° Endpoint: ${endpoint}`);

      // Call Gemini API
      const response = await fetch(`${endpoint}?key=${apiKey}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          contents: [{
            parts: [{
              text: prompt
            }]
          }],
          generationConfig: {
            temperature: 0.2,
            topK: 40,
            topP: 0.95,
            maxOutputTokens: 8192,
          },
          safetySettings: [
            {
              category: "HARM_CATEGORY_HARASSMENT",
              threshold: "BLOCK_NONE"
            },
            {
              category: "HARM_CATEGORY_HATE_SPEECH",
              threshold: "BLOCK_NONE"
            },
            {
              category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              threshold: "BLOCK_NONE"
            },
            {
              category: "HARM_CATEGORY_DANGEROUS_CONTENT",
              threshold: "BLOCK_NONE"
            }
          ]
        })
      });

      if (onProgress) onProgress(70);

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const errorMsg = errorData.error?.message || response.statusText;
        
        // Handle quota/rate limit errors (429)
        if (response.status === 429) {
          // Extract retry time if available
          const retryMatch = errorMsg.match(/retry in ([\d.]+)s/);
          const retrySeconds = retryMatch ? Math.ceil(parseFloat(retryMatch[1])) : 60;
          const retryMinutes = Math.ceil(retrySeconds / 60);

          // Check if it's daily quota or rate limit
          if (errorMsg.includes('quota') || errorMsg.includes('250000')) {
            throw new Error(
              `üö´ ƒê√£ v∆∞·ª£t h·∫°n m·ª©c mi·ªÖn ph√≠ c·ªßa Gemini API\n\n` +
              `üìä H·∫°n m·ª©c free tier: 250,000 tokens/ng√†y\n` +
              `‚è∞ Th·ªùi gian reset: Sau ${retrySeconds}s (~ ${retryMinutes} ph√∫t)\n\n` +
              `üí° Gi·∫£i ph√°p:\n` +
              `1Ô∏è‚É£ ƒê·ª£i ${retryMinutes} ph√∫t r·ªìi th·ª≠ l·∫°i\n` +
              `2Ô∏è‚É£ X·ª≠ l√Ω √≠t segments h∆°n (ch·ªçn ƒëo·∫°n quan tr·ªçng ƒë·ªÉ chu·∫©n h√≥a)\n` +
              `3Ô∏è‚É£ N√¢ng c·∫•p l√™n Gemini API tr·∫£ ph√≠:\n` +
              `   ‚Ä¢ Truy c·∫≠p: https://console.cloud.google.com\n` +
              `   ‚Ä¢ Enable billing ƒë·ªÉ c√≥ quota cao h∆°n (60 requests/ph√∫t)\n\n` +
              `üìà Monitor usage: https://ai.dev/rate-limit\n\n` +
              `Chi ti·∫øt: ${errorMsg}`
            );
          } else {
            // Rate limit (RPM)
            throw new Error(
              `‚è±Ô∏è V∆∞·ª£t gi·ªõi h·∫°n requests/ph√∫t\n\n` +
              `üìä H·∫°n m·ª©c: 15 requests/ph√∫t (free tier)\n` +
              `‚è∞ Th·ª≠ l·∫°i sau: ${retrySeconds}s\n\n` +
              `üí° Gi·∫£i ph√°p: ƒê·ª£i ${Math.ceil(retrySeconds / 60)} ph√∫t r·ªìi th·ª≠ l·∫°i\n\n` +
              `Chi ti·∫øt: ${errorMsg}`
            );
          }
        }
        
        // Provide helpful error messages for other errors
        if (response.status === 403) {
          if (errorMsg.includes('API has not been used') || errorMsg.includes('SERVICE_DISABLED')) {
            throw new Error(
              'API Key kh√¥ng h·ª£p l·ªá ho·∫∑c ch∆∞a enable.\n\n' +
              '‚úÖ L·∫•y API key mi·ªÖn ph√≠ t·∫°i: https://aistudio.google.com/app/apikey\n' +
              'Sau ƒë√≥ paste v√†o Settings ‚Üí Gemini API Key'
            );
          } else if (errorMsg.includes('API_KEY_INVALID')) {
            throw new Error('API Key kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra l·∫°i trong Settings.');
          }
        } else if (response.status === 404) {
          throw new Error(
            `Model "${modelName}" kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng kh·∫£ d·ª•ng.\n\n` +
            'Gi·∫£i ph√°p:\n' +
            '1. M·ªü Settings ‚Üí Click n√∫t "T·∫£i l·∫°i" b√™n c·∫°nh Gemini Model\n' +
            '2. Ch·ªçn model kh√°c t·ª´ danh s√°ch (khuy√™n d√πng: Gemini 2.5 Flash)\n' +
            '3. L∆∞u v√† th·ª≠ l·∫°i\n\n' +
            `Chi ti·∫øt l·ªói: ${errorMsg}`
          );
        }
        
        throw new Error(`Gemini API error (${response.status}): ${errorMsg}`);
      }

      const result = await response.json();
      
      if (onProgress) onProgress(90);

      // Parse AI response
      const refinedSegments = this.parseAIResponse(result);

      if (onProgress) onProgress(100);

      console.log(`‚úÖ Successfully refined ${refinedSegments.length} segments`);
      return refinedSegments;

    } catch (error: any) {
      console.error('AI Refinement Error:', error);
      throw new Error(`Failed to refine transcripts: ${error.message}`);
    }
  }

  /**
   * Create refinement prompt for AI
   * @param transcriptData - Primary data from transcriptions (user-edited)
   * @param rawMetadata - Optional raw data for reference
   */
  private static createRefinementPrompt(transcriptData: any[], rawMetadata: any[] | null): string {
    const dataJson = JSON.stringify(transcriptData, null, 2);
    const hasRawData = rawMetadata && rawMetadata.length > 0;
    const rawDataJson = hasRawData ? JSON.stringify(rawMetadata, null, 2) : null;

    return `Vai tr√≤: B·∫°n l√† m·ªôt th∆∞ k√Ω chuy√™n nghi·ªáp chuy√™n so·∫°n th·∫£o bi√™n b·∫£n cu·ªôc h·ªçp.

Nhi·ªám v·ª•: T√¥i s·∫Ω cung c·∫•p cho b·∫°n vƒÉn b·∫£n ƒë√£ chuy·ªÉn t·ª´ gi·ªçng n√≥i sang text (c√≥ th·ªÉ c√≥ l·ªói nh·∫≠n di·ªán, l·∫∑p t·ª´, thi·∫øu d·∫•u c√¢u). H√£y th·ª±c hi·ªán:

1. S·ª≠a l·ªói nh·∫≠n di·ªán: Ch·ªânh l·∫°i c√°c t·ª´ b·ªã sai (v√≠ d·ª•: 'th√†nh vi√™n h·ªôi ƒë·ªìng' th√†nh 'H·ªôi ƒë·ªìng th√†nh vi√™n').
2. Lo·∫°i b·ªè t·ª´ th·ª´a: X√≥a c√°c t·ª´ ƒë·ªám nh∆∞ '√†', '·ª´m', 'th√¨', 'l√†', 'm√†' ho·∫∑c c√°c ƒëo·∫°n b·ªã l·∫∑p l·∫°i do ng∆∞·ªùi n√≥i ng·∫≠p ng·ª´ng.
3. Th√™m d·∫•u c√¢u & Vi·∫øt hoa: Ng·∫Øt c√¢u h·ª£p l√Ω, vi·∫øt hoa c√°c danh t·ª´ ri√™ng v√† ch·ª©c danh.
4. Gi·ªØ nguy√™n n·ªôi dung: Tuy·ªát ƒë·ªëi kh√¥ng ƒë∆∞·ª£c th√™m b·ªõt √Ω ki·∫øn ho·∫∑c thay ƒë·ªïi s·∫Øc th√°i c·ªßa ng∆∞·ªùi n√≥i.
5. G·ªôp c√°c ƒëo·∫°n li√™n quan: C√°c ƒëo·∫°n text li·ªÅn nhau n·∫øu c√πng n·ªôi dung th√¨ g·ªôp l·∫°i th√†nh m·ªôt ƒëo·∫°n ho√†n ch·ªânh.

ƒê·ªãnh d·∫°ng tr·∫£ v·ªÅ: 
Tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng JSON array v·ªõi format sau (ch·ªâ JSON, kh√¥ng c√≥ markdown code block):
[
  {
    "timestamp": "2026-01-27T10:30:45.123Z",
    "audioTimeMs": 12345,
    "text": "N·ªôi dung ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† chu·∫©n h√≥a."
  },
  ...
]

=== D·ªÆ LI·ªÜU CH√çNH (ƒê·ªô tin c·∫≠y tuy·ªát ƒë·ªëi - C√≥ th·ªÉ ƒë√£ ƒë∆∞·ª£c ng∆∞·ªùi d√πng ch·ªânh s·ª≠a) ===
${dataJson}
${hasRawData ? `\n=== D·ªÆ LI·ªÜU B·ªî TR·ª¢ (Raw output t·ª´ Google Web Speech API - Ch·ªâ tham kh·∫£o) ===\n${rawDataJson}\n\nCh√∫ √Ω: D·ªØ li·ªáu raw ch·ªâ d√πng ƒë·ªÉ tham kh·∫£o th√™m v·ªÅ confidence v√† metadata g·ªëc. ∆Øu ti√™n s·ª≠ d·ª•ng "D·ªØ li·ªáu ch√≠nh" v√¨ c√≥ th·ªÉ ƒë√£ ƒë∆∞·ª£c ng∆∞·ªùi d√πng edit tr·ª±c ti·∫øp.` : ''}

L∆∞u √Ω: 
- S·ª≠ d·ª•ng vƒÉn b·∫£n t·ª´ "D·ªØ li·ªáu ch√≠nh" l√†m ngu·ªìn ch√≠nh
- Gi·ªØ nguy√™n timestamp v√† audioTimeMs t·ª´ d·ªØ li·ªáu g·ªëc
- G·ªôp c√°c segment c√≥ n·ªôi dung li√™n ti·∫øp th√†nh c√¢u ho√†n ch·ªânh
- Ch·ªâ tr·∫£ v·ªÅ JSON array, kh√¥ng th√™m gi·∫£i th√≠ch`;
  }

  /**
   * Parse AI response and create refined segments
   */
  private static parseAIResponse(apiResponse: any): RefinedSegment[] {
    try {
      // Extract text from Gemini response
      const candidates = apiResponse.candidates;
      if (!candidates || candidates.length === 0) {
        throw new Error('No response from AI');
      }

      const content = candidates[0].content;
      if (!content || !content.parts || content.parts.length === 0) {
        throw new Error('Invalid AI response format');
      }

      let responseText = content.parts[0].text.trim();

      // Remove markdown code blocks if present
      responseText = responseText.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();

      // Parse JSON
      const refinedData = JSON.parse(responseText);

      if (!Array.isArray(refinedData)) {
        throw new Error('AI response is not an array');
      }

      // Validate and map to RefinedSegment
      const segments: RefinedSegment[] = refinedData
        .filter(item => item.text && item.text.trim().length > 0)
        .map(item => ({
          text: item.text.trim(),
          timestamp: item.timestamp || new Date().toISOString(),
          audioTimeMs: item.audioTimeMs
        }));

      return segments;

    } catch (error: any) {
      console.error('Failed to parse AI response:', error);
      console.log('Raw API response:', JSON.stringify(apiResponse, null, 2));
      throw new Error(`Failed to parse AI response: ${error.message}`);
    }
  }

  /**
   * Format audio time in milliseconds to mm:ss
   */
  private static formatAudioTime(ms: number): string {
    const totalSeconds = Math.floor(ms / 1000);
    const minutes = Math.floor(totalSeconds / 60);
    const seconds = totalSeconds % 60;
    return `${minutes}:${String(seconds).padStart(2, '0')}`;
  }

  /**
   * Convert refined segments back to TranscriptionResult format
   */
  public static convertToTranscriptionResults(
    refinedSegments: RefinedSegment[],
    speakerPrefix: string = 'Person1'
  ): TranscriptionResult[] {
    return refinedSegments.map((segment, index) => ({
      id: `refined-${Date.now()}-${index}`,
      text: segment.text,
      startTime: segment.timestamp,
      endTime: segment.timestamp, // Same as start for refined segments
      audioTimeMs: segment.audioTimeMs,
      confidence: 1.0, // AI-refined content has high confidence
      speaker: speakerPrefix,
      isFinal: true,
      isManuallyEdited: false,
      isAIRefined: true // Mark as AI-refined
    }));
  }

  /**
   * Get file size limit based on API tier
   * Free tier: 20MB, Paid tier: Could be higher (check with Gemini docs)
   * TODO: Add ability to configure this in Settings for paid users
   */
  private static getFileSizeLimit(): { maxSizeMB: number; tier: string } {
    // For now, use conservative 20MB limit (Gemini free tier)
    // Future: Detect paid tier or allow user to configure in Settings
    return { maxSizeMB: 20, tier: 'free' };
  }

  /**
   * Transcribe audio file using Gemini Multimodal API
   * Gemini 1.5+ models can directly process audio files (mp3, wav, aac, webm, etc.)
   */
  public static async transcribeAudioWithGemini(
    apiKey: string,
    audioBlob: Blob,
    modelName: string, // e.g., "models/gemini-1.5-flash" or "models/gemini-2.0-flash-exp"
    onProgress?: (progress: number) => void,
    skipSizeCheck: boolean = false // Skip size check when called from auto-split flow
  ): Promise<TranscriptionResult[]> {
    if (!apiKey || apiKey.trim().length === 0) {
      throw new Error('Gemini API Key is required');
    }

    if (!modelName || !modelName.startsWith('models/')) {
      throw new Error('Please select a Gemini model in Settings');
    }

    // Validate file size (limit depends on API tier)
    // Skip this check when called from transcribeEntireAudioWithGemini (already split into valid chunks)
    if (!skipSizeCheck) {
      const { maxSizeMB, tier } = this.getFileSizeLimit();
      const MAX_FILE_SIZE = maxSizeMB * 1024 * 1024;
      const fileSizeMB = audioBlob.size / (1024 * 1024);
      
      console.log(`üìä File size: ${fileSizeMB.toFixed(2)} MB (Limit: ${maxSizeMB} MB, Tier: ${tier})`);
      
      if (audioBlob.size > MAX_FILE_SIZE) {
        // Return special error object with file size info
        const error: any = new Error('FILE_TOO_LARGE');
        error.fileSizeMB = fileSizeMB;
        error.maxSizeMB = maxSizeMB;
        error.tier = tier;
        throw error;
      }
    }

    if (onProgress) onProgress(10);

    try {
      // Convert WebM to WAV if needed, with size optimization
      let processedAudio = audioBlob;
      const needsConversion = audioBlob.type === 'audio/webm' || audioBlob.type === 'video/webm';
      
      if (needsConversion) {
        console.log('üîÑ Converting WebM to WAV...');
        if (onProgress) onProgress(15);
        
        const originalSizeMB = audioBlob.size / (1024 * 1024);
        
        // Convert with lower sample rate if file is large
        const targetSampleRate = audioBlob.size > 10 * 1024 * 1024 ? 16000 : 44100;
        processedAudio = await this.convertToWav(audioBlob, targetSampleRate);
        
        const newSizeMB = processedAudio.size / (1024 * 1024);
        console.log(`‚úÖ Converted: ${originalSizeMB.toFixed(2)}MB ‚Üí ${newSizeMB.toFixed(2)}MB`);
        
        // Check again after conversion (only if not skipping size check)
        if (!skipSizeCheck) {
          const { maxSizeMB } = this.getFileSizeLimit();
          const MAX_FILE_SIZE = maxSizeMB * 1024 * 1024;
          
          if (processedAudio.size > MAX_FILE_SIZE) {
            throw new Error(
              `‚ùå Sau chuy·ªÉn ƒë·ªïi, file v·∫´n qu√° l·ªõn: ${newSizeMB.toFixed(2)} MB\n\n` +
              `Vui l√≤ng gi·∫£m th·ªùi l∆∞·ª£ng ghi √¢m ho·∫∑c gi·∫£m ch·∫•t l∆∞·ª£ng.`
            );
          }
        }
        
        if (onProgress) onProgress(25);
      }

      // Convert audio blob to base64
      const base64Audio = await this.blobToBase64(processedAudio);
      if (onProgress) onProgress(30);

      // Get MIME type (use WAV if converted)
      const mimeType = processedAudio.type || 'audio/wav';

      // Prepare request
      const endpoint = `https://generativelanguage.googleapis.com/${this.GEMINI_API_VERSION}/${modelName}:generateContent?key=${apiKey}`;

      const requestBody = {
        contents: [{
          parts: [
            {
              text: `H√£y nghe file √¢m thanh cu·ªôc h·ªçp n√†y v√† chuy·ªÉn th√†nh vƒÉn b·∫£n. Y√™u c·∫ßu b·∫Øt bu·ªôc:

1. Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n h·ªôi tho·∫°i t·ª± nhi√™n.
2. G·∫Øn nh√£n th·ªùi gian [mm:ss] v√†o ƒë·∫ßu m·ªói ƒëo·∫°n d·ª±a tr√™n th·ªùi ƒëi·ªÉm ng∆∞·ªùi n√≥i b·∫Øt ƒë·∫ßu trong file √¢m thanh.
3. N·∫øu c√≥ nhi·ªÅu ng∆∞·ªùi n√≥i, h√£y ph√¢n bi·ªát b·∫±ng c√°ch ghi 'Ng∆∞·ªùi n√≥i 1:', 'Ng∆∞·ªùi n√≥i 2:'...
4. L√†m s·∫°ch vƒÉn b·∫£n (lo·∫°i b·ªè t·ª´ ƒë·ªám, s·ª≠a l·ªói ch√≠nh t·∫£).
5. ƒê·ªãnh d·∫°ng: Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON v·ªõi c·∫•u tr√∫c:
{
  "segments": [
    {
      "timestamp": "mm:ss",
      "speaker": "Ng∆∞·ªùi n√≥i 1",
      "text": "n·ªôi dung"
    }
  ]
}`
            },
            {
              inline_data: {
                mime_type: mimeType,
                data: base64Audio
              }
            }
          ]
        }]
      };

      if (onProgress) onProgress(40);

      // Make API request
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody)
      });

      if (onProgress) onProgress(70);

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(
          `Gemini API error (${response.status}): ${errorData.error?.message || response.statusText}`
        );
      }

      const data = await response.json();
      if (onProgress) onProgress(90);

      // Parse response
      const results = this.parseGeminiAudioTranscription(data);
      if (onProgress) onProgress(100);

      return results;

    } catch (error: any) {
      console.error('Gemini audio transcription error:', error);
      throw new Error(`Failed to transcribe audio: ${error.message}`);
    }
  }

  /**
   * Convert Blob to Base64 string
   */
  private static blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        const base64 = (reader.result as string).split(',')[1]; // Remove data:audio/...;base64, prefix
        resolve(base64);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  /**
   * Convert audio blob to WAV format with optional sample rate optimization
   * Gemini API requires WAV or MP3 format, not WebM
   * @param targetSampleRate - Target sample rate (16000 for smaller files, 44100 for quality)
   */
  private static async convertToWav(audioBlob: Blob, targetSampleRate: number = 44100): Promise<Blob> {
    return new Promise((resolve, reject) => {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const reader = new FileReader();

      reader.onload = async (e) => {
        try {
          const arrayBuffer = e.target?.result as ArrayBuffer;
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

          // Resample if needed to reduce file size
          let finalBuffer = audioBuffer;
          if (audioBuffer.sampleRate !== targetSampleRate) {
            console.log(`üîä Resampling: ${audioBuffer.sampleRate}Hz ‚Üí ${targetSampleRate}Hz`);
            finalBuffer = await this.resampleAudioBuffer(audioBuffer, targetSampleRate);
          }

          // Convert to WAV
          const wavBlob = this.audioBufferToWav(finalBuffer);
          resolve(wavBlob);
        } catch (error) {
          reject(error);
        }
      };

      reader.onerror = reject;
      reader.readAsArrayBuffer(audioBlob);
    });
  }

  /**
   * Resample AudioBuffer to target sample rate (reduces file size)
   */
  private static async resampleAudioBuffer(audioBuffer: AudioBuffer, targetSampleRate: number): Promise<AudioBuffer> {
    const offlineContext = new OfflineAudioContext(
      audioBuffer.numberOfChannels,
      audioBuffer.duration * targetSampleRate,
      targetSampleRate
    );

    const source = offlineContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(offlineContext.destination);
    source.start();

    return await offlineContext.startRendering();
  }

  /**
   * Convert AudioBuffer to WAV Blob
   */
  private static audioBufferToWav(audioBuffer: AudioBuffer): Blob {
    const numberOfChannels = audioBuffer.numberOfChannels;
    const sampleRate = audioBuffer.sampleRate;
    const format = 1; // PCM
    const bitDepth = 16;

    const bytesPerSample = bitDepth / 8;
    const blockAlign = numberOfChannels * bytesPerSample;

    const data = [];
    for (let i = 0; i < audioBuffer.numberOfChannels; i++) {
      data.push(audioBuffer.getChannelData(i));
    }

    const interleaved = this.interleave(data);
    const dataLength = interleaved.length * bytesPerSample;
    const buffer = new ArrayBuffer(44 + dataLength);
    const view = new DataView(buffer);

    // Write WAV header
    this.writeString(view, 0, 'RIFF');
    view.setUint32(4, 36 + dataLength, true);
    this.writeString(view, 8, 'WAVE');
    this.writeString(view, 12, 'fmt ');
    view.setUint32(16, 16, true); // fmt chunk size
    view.setUint16(20, format, true);
    view.setUint16(22, numberOfChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * blockAlign, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bitDepth, true);
    this.writeString(view, 36, 'data');
    view.setUint32(40, dataLength, true);

    // Write audio data
    this.floatTo16BitPCM(view, 44, interleaved);

    return new Blob([buffer], { type: 'audio/wav' });
  }

  /**
   * Interleave multiple audio channels
   */
  private static interleave(channelData: Float32Array[]): Float32Array {
    const length = channelData[0].length;
    const numberOfChannels = channelData.length;
    const result = new Float32Array(length * numberOfChannels);

    let offset = 0;
    for (let i = 0; i < length; i++) {
      for (let channel = 0; channel < numberOfChannels; channel++) {
        result[offset++] = channelData[channel][i];
      }
    }

    return result;
  }
  /**
   * Split audio blob into chunks for batch processing
   * IMPORTANT: Should receive WAV blob (already converted), not original WebM
   * Calculates chunk duration to keep each chunk under size limit
   * @param audioBlob - Audio blob (preferably WAV for accurate size calculation)
   * @param maxChunkSizeMB - Maximum chunk size in MB (default: 20MB for free tier)
   * @returns Promise<Array> - Array of { blob, startTimeMs, endTimeMs }
   */
  public static async splitAudioIntoChunks(
    audioBlob: Blob,
    maxChunkSizeMB: number = 20
  ): Promise<{ blob: Blob; startTimeMs: number; endTimeMs: number }[]> {
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
    const reader = new FileReader();

    return new Promise((resolve, reject) => {
      reader.onload = async (e) => {
        try {
          const arrayBuffer = e.target?.result as ArrayBuffer;
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

          const totalDurationMs = audioBuffer.duration * 1000;
          const totalSizeMB = audioBlob.size / (1024 * 1024);

          // Calculate number of chunks needed
          const numberOfChunks = Math.ceil(totalSizeMB / maxChunkSizeMB);
          const chunkDurationMs = totalDurationMs / numberOfChunks;

          console.log(`üì¶ Splitting audio: ${totalSizeMB.toFixed(2)}MB into ${numberOfChunks} chunks`);

          const chunks: { blob: Blob; startTimeMs: number; endTimeMs: number }[] = [];

          for (let i = 0; i < numberOfChunks; i++) {
            const startTimeMs = i * chunkDurationMs;
            const endTimeMs = Math.min((i + 1) * chunkDurationMs, totalDurationMs);

            console.log(`‚è±Ô∏è Extracting chunk ${i + 1}/${numberOfChunks}: ${startTimeMs.toFixed(0)}ms - ${endTimeMs.toFixed(0)}ms`);

            const chunkBlob = await this.extractAudioSegment(audioBlob, startTimeMs, endTimeMs);

            chunks.push({
              blob: chunkBlob,
              startTimeMs,
              endTimeMs
            });
          }

          resolve(chunks);
        } catch (error) {
          reject(error);
        }
      };

      reader.onerror = reject;
      reader.readAsArrayBuffer(audioBlob);
    });
  }

  /**
   * Extract a segment from audio blob based on time range
   * @param audioBlob - Original audio blob
   * @param startTimeMs - Start time in milliseconds
   * @param endTimeMs - End time in milliseconds
   * @returns Promise<Blob> - Audio segment blob
   */
  public static async extractAudioSegment(
    audioBlob: Blob,
    startTimeMs: number,
    endTimeMs: number
  ): Promise<Blob> {
    return new Promise((resolve, reject) => {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const reader = new FileReader();

      reader.onload = async (e) => {
        try {
          const arrayBuffer = e.target?.result as ArrayBuffer;
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

          // Calculate start and end in samples
          const startSample = Math.floor((startTimeMs / 1000) * audioBuffer.sampleRate);
          const endSample = Math.floor((endTimeMs / 1000) * audioBuffer.sampleRate);
          const segmentLength = endSample - startSample;

          // Create new buffer for the segment
          const segmentBuffer = audioContext.createBuffer(
            audioBuffer.numberOfChannels,
            segmentLength,
            audioBuffer.sampleRate
          );

          // Copy data for each channel
          for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
            const channelData = audioBuffer.getChannelData(channel);
            const segmentData = segmentBuffer.getChannelData(channel);
            for (let i = 0; i < segmentLength; i++) {
              segmentData[i] = channelData[startSample + i];
            }
          }

          // Convert to WAV
          const wavBlob = this.audioBufferToWav(segmentBuffer);
          resolve(wavBlob);
        } catch (error) {
          reject(error);
        }
      };

      reader.onerror = reject;
      reader.readAsArrayBuffer(audioBlob);
    });
  }

  /**
   * Process entire audio file by automatically splitting into chunks
   * Respects Gemini API limits: 15 req/min, 1500 req/day, 20MB per file
   * @param apiKey - Gemini API key
   * @param audioBlob - Original audio blob (can be > 20MB)
   * @param modelName - Gemini model name
   * @param onProgress - Progress callback (progress: number, message: string)
   * @returns Promise<TranscriptionResult[]> - All transcription results, sorted by timestamp
   */
  public static async transcribeEntireAudioWithGemini(
    apiKey: string,
    audioBlob: Blob,
    modelName: string,
    onProgress?: (progress: number, message: string) => void
  ): Promise<TranscriptionResult[]> {
    const { maxSizeMB } = this.getFileSizeLimit();

    // CRITICAL: Convert to WAV first, THEN split based on WAV size
    // Because Gemini calculates size based on WAV, not original WebM
    if (onProgress) onProgress(3, 'ƒêang chuy·ªÉn ƒë·ªïi sang ƒë·ªãnh d·∫°ng WAV...');
    
    let wavBlob = audioBlob;
    const needsConversion = audioBlob.type === 'audio/webm' || audioBlob.type === 'video/webm';
    
    if (needsConversion) {
      // Convert with lower sample rate for smaller file size
      const targetSampleRate = 16000; // Lower sample rate = smaller file
      wavBlob = await this.convertToWav(audioBlob, targetSampleRate);
      
      const originalSizeMB = audioBlob.size / (1024 * 1024);
      const wavSizeMB = wavBlob.size / (1024 * 1024);
      console.log(`üîÑ Converted: ${originalSizeMB.toFixed(2)}MB (WebM) ‚Üí ${wavSizeMB.toFixed(2)}MB (WAV)`);
    }

    // Now split the WAV file into chunks based on actual WAV size
    if (onProgress) onProgress(8, 'ƒêang ph√¢n t√≠ch v√† chia file WAV...');
    const chunks = await this.splitAudioIntoChunks(wavBlob, maxSizeMB);

    if (onProgress) onProgress(10, `ƒê√£ chia th√†nh ${chunks.length} ph·∫ßn. B·∫Øt ƒë·∫ßu chuy·ªÉn ƒë·ªïi...`);

    const allResults: TranscriptionResult[] = [];

    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      const chunkProgress = 10 + ((i / chunks.length) * 80);

      if (onProgress) {
        onProgress(
          chunkProgress,
          `ƒêang x·ª≠ l√Ω ph·∫ßn ${i + 1}/${chunks.length}...`
        );
      }

      try {
        // Transcribe this chunk (skip size check - already validated and split)
        const results = await this.transcribeAudioWithGemini(
          apiKey,
          chunk.blob,
          modelName,
          (subProgress) => {
            if (onProgress) {
              const totalProgress = chunkProgress + (subProgress / chunks.length) * 0.8;
              onProgress(totalProgress, `Ph·∫ßn ${i + 1}/${chunks.length}: ${subProgress.toFixed(0)}%`);
            }
          },
          true // skipSizeCheck = true (chunks already validated)
        );

        // Adjust timestamps for this chunk
        const adjustedResults = this.adjustTimestamps(results, chunk.startTimeMs);
        allResults.push(...adjustedResults);

        // Add delay between chunks to respect rate limits (15 req/min)
        if (i < chunks.length - 1) {
          const delaySeconds = 5; // 5 seconds = max 12 req/min (safe)
          if (onProgress) {
            onProgress(
              chunkProgress + 5,
              `ƒê·ª£i ${delaySeconds}s tr∆∞·ªõc khi x·ª≠ l√Ω ph·∫ßn ti·∫øp theo...`
            );
          }
          await new Promise(resolve => setTimeout(resolve, delaySeconds * 1000));
        }
      } catch (error: any) {
        // Handle quota errors
        if (error.message.includes('429') || error.message.includes('quota')) {
          throw new Error(
            `V∆∞·ª£t h·∫°n m·ª©c API t·∫°i ph·∫ßn ${i + 1}/${chunks.length}.\n\n` +
            `‚úÖ ƒê√£ x·ª≠ l√Ω: ${i}/${chunks.length} ph·∫ßn\n` +
            `‚ùå L·ªói: ${error.message}\n\n` +
            `üí° ƒê·ª£i 24 gi·ªù ho·∫∑c n√¢ng c·∫•p Paid tier.`
          );
        }
        throw error;
      }
    }

    // Sort by timestamp
    allResults.sort((a, b) => (a.audioTimeMs || 0) - (b.audioTimeMs || 0));

    if (onProgress) onProgress(100, `Ho√†n th√†nh! ${allResults.length} segments`);

    console.log(`‚úÖ Transcribed entire audio: ${allResults.length} segments from ${chunks.length} chunks`);
    return allResults;
  }

  /**
   * Adjust timestamps in transcription results based on segment start time
   * @param results - Transcription results from segment
   * @param offsetMs - Offset in milliseconds (segment start time)
   * @returns Adjusted transcription results
   */
  public static adjustTimestamps(
    results: TranscriptionResult[],
    offsetMs: number
  ): TranscriptionResult[] {
    return results.map(result => ({
      ...result,
      audioTimeMs: result.audioTimeMs ? result.audioTimeMs + offsetMs : undefined
    }));
  }

  /*
   * FUTURE ENHANCEMENT: optimizeAudioFormat() method
   * Could convert WAV to WebM/Opus to reduce file size by 70-90%
   * Reserved for future implementation
   */
  /**
   * Write string to DataView
   */
  private static writeString(view: DataView, offset: number, string: string): void {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i));
    }
  }

  /**
   * Convert Float32 samples to 16-bit PCM
   */
  private static floatTo16BitPCM(view: DataView, offset: number, input: Float32Array): void {
    for (let i = 0; i < input.length; i++, offset += 2) {
      const s = Math.max(-1, Math.min(1, input[i]));
      view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
    }
  }

  /**
   * Parse Gemini audio transcription response
   */
  private static parseGeminiAudioTranscription(apiResponse: any): TranscriptionResult[] {
    try {
      const candidates = apiResponse.candidates;
      if (!candidates || candidates.length === 0) {
        throw new Error('No response from Gemini API');
      }

      const content = candidates[0].content;
      if (!content || !content.parts || content.parts.length === 0) {
        throw new Error('Empty response from Gemini');
      }

      const textResponse = content.parts[0].text;
      if (!textResponse) {
        throw new Error('No text in Gemini response');
      }

      // Extract JSON from response (handle markdown code blocks)
      let jsonText = textResponse.trim();
      const jsonMatch = jsonText.match(/```json\s*([\s\S]*?)```/) || jsonText.match(/```\s*([\s\S]*?)```/);
      if (jsonMatch) {
        jsonText = jsonMatch[1].trim();
      }

      const parsed = JSON.parse(jsonText);

      if (!parsed.segments || !Array.isArray(parsed.segments)) {
        throw new Error('Invalid JSON structure: missing segments array');
      }

      // Convert to TranscriptionResult format
      return parsed.segments.map((segment: any, index: number) => {
        const timestamp = segment.timestamp || '0:00';
        const audioTimeMs = this.parseTimestampToMs(timestamp);

        return {
          id: `gemini-${Date.now()}-${index}`,
          text: segment.text || '',
          startTime: new Date().toLocaleString('vi-VN'),
          endTime: new Date().toLocaleString('vi-VN'),
          audioTimeMs,
          confidence: 1.0,
          speaker: segment.speaker || 'Unknown',
          isFinal: true,
          isManuallyEdited: false,
          isAIRefined: true
        };
      });

    } catch (error: any) {
      console.error('Failed to parse Gemini audio transcription:', error);
      console.log('Raw API response:', JSON.stringify(apiResponse, null, 2));
      throw new Error(`Failed to parse Gemini response: ${error.message}`);
    }
  }

  /**
   * Parse timestamp string (mm:ss or m:ss) to milliseconds
   */
  private static parseTimestampToMs(timestamp: string): number {
    try {
      const parts = timestamp.split(':').map(p => parseInt(p.trim(), 10));
      if (parts.length === 2) {
        const [minutes, seconds] = parts;
        return (minutes * 60 + seconds) * 1000;
      }
      return 0;
    } catch {
      return 0;
    }
  }
}
