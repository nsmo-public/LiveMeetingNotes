import type { TranscriptionResult } from '../types/types';

export interface RawTranscriptData {
  text: string;
  timestamp: string;
  audioTimeMs?: number;
  confidence: number;
  isFinal: boolean;
}

export interface RefinedSegment {
  text: string;
  timestamp: string;
  audioTimeMs?: number;
}

/**
 * AI Refinement Service for Gemini AI
 * Refines raw speech-to-text transcripts with AI
 */
export class AIRefinementService {
  private static readonly GEMINI_MODELS_ENDPOINT = 'https://generativelanguage.googleapis.com/v1beta/models';
  private static readonly GEMINI_API_VERSION = 'v1beta'; // Use v1beta as it's more stable

  /**
   * List available Gemini models for the given API key
   * Useful for debugging and verifying API key access
   */
  public static async listGeminiModels(apiKey: string): Promise<any> {
    if (!apiKey || apiKey.trim().length === 0) {
      throw new Error('API Key is required');
    }

    try {
      const response = await fetch(`${this.GEMINI_MODELS_ENDPOINT}?key=${apiKey}`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        }
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(
          `Failed to list models (${response.status}): ${errorData.error?.message || response.statusText}\n` +
          `URL: ${this.GEMINI_MODELS_ENDPOINT}`
        );
      }

      const data = await response.json();
      return data;
    } catch (error: any) {
      console.error('Error listing Gemini models:', error);
      throw new Error(`Cannot list Gemini models: ${error.message}`);
    }
  }

  /**
   * Refine transcripts using Gemini AI
   * @param transcriptions - Primary data (user-edited, highest reliability)
   * @param rawData - Supplementary data (original Web Speech API output for reference)
   */
  public static async refineTranscripts(
    apiKey: string,
    transcriptions: TranscriptionResult[], // Primary data source
    rawData: RawTranscriptData[], // Optional: supplementary raw data
    modelName: string, // REQUIRED: specific Gemini model (e.g., "models/gemini-2.5-flash")
    onProgress?: (progress: number) => void
  ): Promise<RefinedSegment[]> {
    return this.refineWithGemini(apiKey, transcriptions, rawData, modelName, onProgress);
  }

  /**
   * Refine with Google Gemini API using user-selected model
   */
  private static async refineWithGemini(
    apiKey: string,
    transcriptions: TranscriptionResult[], // Primary data
    rawData: RawTranscriptData[], // Supplementary data
    modelName: string, // REQUIRED: specific model like "models/gemini-2.5-flash"
    onProgress?: (progress: number) => void
  ): Promise<RefinedSegment[]> {
    if (!apiKey || apiKey.trim().length === 0) {
      throw new Error('API Key is required for AI refinement');
    }

    if (transcriptions.length === 0) {
      throw new Error('No transcript data to refine');
    }

    // Validate model name
    if (!modelName || !modelName.trim() || !modelName.startsWith('models/')) {
      throw new Error(
        'Vui lÃ²ng chá»n Gemini Model trong Settings.\n\n' +
        'BÆ°á»›c 1: Má»Ÿ Settings â†’ Nháº­p Gemini API Key\n' +
        'BÆ°á»›c 2: Chá» há»‡ thá»‘ng táº£i danh sÃ¡ch models\n' +
        'BÆ°á»›c 3: Chá»n model tá»« dropdown (vÃ­ dá»¥: Gemini 2.5 Flash)\n' +
        'BÆ°á»›c 4: LÆ°u vÃ  thá»­ láº¡i'
      );
    }

    try {
      // Prepare primary data from transcriptions (user-edited, highest reliability)
      const transcriptData = transcriptions.map((item, index) => ({
        index: index + 1,
        timestamp: item.startTime,
        audioTime: item.audioTimeMs !== undefined ? this.formatAudioTime(item.audioTimeMs) : undefined,
        text: item.text,
        confidence: item.confidence,
        type: item.isFinal ? 'final' : 'interim'
      }));

      // Prepare supplementary raw data (if available)
      const hasRawData = rawData && rawData.length > 0;
      const rawMetadata = hasRawData ? rawData.map((item, index) => ({
        index: index + 1,
        timestamp: item.timestamp,
        audioTime: item.audioTimeMs !== undefined ? this.formatAudioTime(item.audioTimeMs) : undefined,
        text: item.text,
        confidence: item.confidence,
        type: item.isFinal ? 'final' : 'interim'
      })) : null;

      // Create prompt
      const prompt = this.createRefinementPrompt(transcriptData, rawMetadata);

      if (onProgress) onProgress(10);

      // Build endpoint URL with selected model
      const endpoint = `https://generativelanguage.googleapis.com/${this.GEMINI_API_VERSION}/${modelName}:generateContent`;
      console.log(`ðŸ¤– Using Gemini model: ${modelName}`);
      console.log(`ðŸ“¡ Endpoint: ${endpoint}`);

      // Call Gemini API
      const response = await fetch(`${endpoint}?key=${apiKey}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          contents: [{
            parts: [{
              text: prompt
            }]
          }],
          generationConfig: {
            temperature: 0.2,
            topK: 40,
            topP: 0.95,
            maxOutputTokens: 8192,
          },
          safetySettings: [
            {
              category: "HARM_CATEGORY_HARASSMENT",
              threshold: "BLOCK_NONE"
            },
            {
              category: "HARM_CATEGORY_HATE_SPEECH",
              threshold: "BLOCK_NONE"
            },
            {
              category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              threshold: "BLOCK_NONE"
            },
            {
              category: "HARM_CATEGORY_DANGEROUS_CONTENT",
              threshold: "BLOCK_NONE"
            }
          ]
        })
      });

      if (onProgress) onProgress(70);

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const errorMsg = errorData.error?.message || response.statusText;
        
        // Provide helpful error messages
        if (response.status === 403) {
          if (errorMsg.includes('API has not been used') || errorMsg.includes('SERVICE_DISABLED')) {
            throw new Error(
              'API Key khÃ´ng há»£p lá»‡ hoáº·c chÆ°a enable.\n\n' +
              'âœ… Láº¥y API key miá»…n phÃ­ táº¡i: https://aistudio.google.com/app/apikey\n' +
              'Sau Ä‘Ã³ paste vÃ o Settings â†’ Gemini API Key'
            );
          } else if (errorMsg.includes('API_KEY_INVALID')) {
            throw new Error('API Key khÃ´ng há»£p lá»‡. Vui lÃ²ng kiá»ƒm tra láº¡i trong Settings.');
          }
        } else if (response.status === 404) {
          throw new Error(
            `Model "${modelName}" khÃ´ng tá»“n táº¡i hoáº·c khÃ´ng kháº£ dá»¥ng.\n\n` +
            'Giáº£i phÃ¡p:\n' +
            '1. Má»Ÿ Settings â†’ Click nÃºt "Táº£i láº¡i" bÃªn cáº¡nh Gemini Model\n' +
            '2. Chá»n model khÃ¡c tá»« danh sÃ¡ch (khuyÃªn dÃ¹ng: Gemini 2.5 Flash)\n' +
            '3. LÆ°u vÃ  thá»­ láº¡i\n\n' +
            `Chi tiáº¿t lá»—i: ${errorMsg}`
          );
        }
        
        throw new Error(`Gemini API error (${response.status}): ${errorMsg}`);
      }

      const result = await response.json();
      
      if (onProgress) onProgress(90);

      // Parse AI response
      const refinedSegments = this.parseAIResponse(result);

      if (onProgress) onProgress(100);

      console.log(`âœ… Successfully refined ${refinedSegments.length} segments`);
      return refinedSegments;

    } catch (error: any) {
      console.error('AI Refinement Error:', error);
      throw new Error(`Failed to refine transcripts: ${error.message}`);
    }
  }

  /**
   * Create refinement prompt for AI
   * @param transcriptData - Primary data from transcriptions (user-edited)
   * @param rawMetadata - Optional raw data for reference
   */
  private static createRefinementPrompt(transcriptData: any[], rawMetadata: any[] | null): string {
    const dataJson = JSON.stringify(transcriptData, null, 2);
    const hasRawData = rawMetadata && rawMetadata.length > 0;
    const rawDataJson = hasRawData ? JSON.stringify(rawMetadata, null, 2) : null;

    return `Vai trÃ²: Báº¡n lÃ  má»™t thÆ° kÃ½ chuyÃªn nghiá»‡p chuyÃªn soáº¡n tháº£o biÃªn báº£n cuá»™c há»p.

Nhiá»‡m vá»¥: TÃ´i sáº½ cung cáº¥p cho báº¡n vÄƒn báº£n Ä‘Ã£ chuyá»ƒn tá»« giá»ng nÃ³i sang text (cÃ³ thá»ƒ cÃ³ lá»—i nháº­n diá»‡n, láº·p tá»«, thiáº¿u dáº¥u cÃ¢u). HÃ£y thá»±c hiá»‡n:

1. Sá»­a lá»—i nháº­n diá»‡n: Chá»‰nh láº¡i cÃ¡c tá»« bá»‹ sai (vÃ­ dá»¥: 'thÃ nh viÃªn há»™i Ä‘á»“ng' thÃ nh 'Há»™i Ä‘á»“ng thÃ nh viÃªn').
2. Loáº¡i bá» tá»« thá»«a: XÃ³a cÃ¡c tá»« Ä‘á»‡m nhÆ° 'Ã ', 'á»«m', 'thÃ¬', 'lÃ ', 'mÃ ' hoáº·c cÃ¡c Ä‘oáº¡n bá»‹ láº·p láº¡i do ngÆ°á»i nÃ³i ngáº­p ngá»«ng.
3. ThÃªm dáº¥u cÃ¢u & Viáº¿t hoa: Ngáº¯t cÃ¢u há»£p lÃ½, viáº¿t hoa cÃ¡c danh tá»« riÃªng vÃ  chá»©c danh.
4. Giá»¯ nguyÃªn ná»™i dung: Tuyá»‡t Ä‘á»‘i khÃ´ng Ä‘Æ°á»£c thÃªm bá»›t Ã½ kiáº¿n hoáº·c thay Ä‘á»•i sáº¯c thÃ¡i cá»§a ngÆ°á»i nÃ³i.
5. Gá»™p cÃ¡c Ä‘oáº¡n liÃªn quan: CÃ¡c Ä‘oáº¡n text liá»n nhau náº¿u cÃ¹ng ná»™i dung thÃ¬ gá»™p láº¡i thÃ nh má»™t Ä‘oáº¡n hoÃ n chá»‰nh.

Äá»‹nh dáº¡ng tráº£ vá»: 
Tráº£ vá» dÆ°á»›i dáº¡ng JSON array vá»›i format sau (chá»‰ JSON, khÃ´ng cÃ³ markdown code block):
[
  {
    "timestamp": "2026-01-27T10:30:45.123Z",
    "audioTimeMs": 12345,
    "text": "Ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch vÃ  chuáº©n hÃ³a."
  },
  ...
]

=== Dá»® LIá»†U CHÃNH (Äá»™ tin cáº­y tuyá»‡t Ä‘á»‘i - CÃ³ thá»ƒ Ä‘Ã£ Ä‘Æ°á»£c ngÆ°á»i dÃ¹ng chá»‰nh sá»­a) ===
${dataJson}
${hasRawData ? `\n=== Dá»® LIá»†U Bá»” TRá»¢ (Raw output tá»« Google Web Speech API - Chá»‰ tham kháº£o) ===\n${rawDataJson}\n\nChÃº Ã½: Dá»¯ liá»‡u raw chá»‰ dÃ¹ng Ä‘á»ƒ tham kháº£o thÃªm vá» confidence vÃ  metadata gá»‘c. Æ¯u tiÃªn sá»­ dá»¥ng "Dá»¯ liá»‡u chÃ­nh" vÃ¬ cÃ³ thá»ƒ Ä‘Ã£ Ä‘Æ°á»£c ngÆ°á»i dÃ¹ng edit trá»±c tiáº¿p.` : ''}

LÆ°u Ã½: 
- Sá»­ dá»¥ng vÄƒn báº£n tá»« "Dá»¯ liá»‡u chÃ­nh" lÃ m nguá»“n chÃ­nh
- Giá»¯ nguyÃªn timestamp vÃ  audioTimeMs tá»« dá»¯ liá»‡u gá»‘c
- Gá»™p cÃ¡c segment cÃ³ ná»™i dung liÃªn tiáº¿p thÃ nh cÃ¢u hoÃ n chá»‰nh
- Chá»‰ tráº£ vá» JSON array, khÃ´ng thÃªm giáº£i thÃ­ch`;
  }

  /**
   * Parse AI response and create refined segments
   */
  private static parseAIResponse(apiResponse: any): RefinedSegment[] {
    try {
      // Extract text from Gemini response
      const candidates = apiResponse.candidates;
      if (!candidates || candidates.length === 0) {
        throw new Error('No response from AI');
      }

      const content = candidates[0].content;
      if (!content || !content.parts || content.parts.length === 0) {
        throw new Error('Invalid AI response format');
      }

      let responseText = content.parts[0].text.trim();

      // Remove markdown code blocks if present
      responseText = responseText.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();

      // Parse JSON
      const refinedData = JSON.parse(responseText);

      if (!Array.isArray(refinedData)) {
        throw new Error('AI response is not an array');
      }

      // Validate and map to RefinedSegment
      const segments: RefinedSegment[] = refinedData
        .filter(item => item.text && item.text.trim().length > 0)
        .map(item => ({
          text: item.text.trim(),
          timestamp: item.timestamp || new Date().toISOString(),
          audioTimeMs: item.audioTimeMs
        }));

      return segments;

    } catch (error: any) {
      console.error('Failed to parse AI response:', error);
      console.log('Raw API response:', JSON.stringify(apiResponse, null, 2));
      throw new Error(`Failed to parse AI response: ${error.message}`);
    }
  }

  /**
   * Format audio time in milliseconds to mm:ss
   */
  private static formatAudioTime(ms: number): string {
    const totalSeconds = Math.floor(ms / 1000);
    const minutes = Math.floor(totalSeconds / 60);
    const seconds = totalSeconds % 60;
    return `${minutes}:${String(seconds).padStart(2, '0')}`;
  }

  /**
   * Convert refined segments back to TranscriptionResult format
   */
  public static convertToTranscriptionResults(
    refinedSegments: RefinedSegment[],
    speakerPrefix: string = 'Person1'
  ): TranscriptionResult[] {
    return refinedSegments.map((segment, index) => ({
      id: `refined-${Date.now()}-${index}`,
      text: segment.text,
      startTime: segment.timestamp,
      endTime: segment.timestamp, // Same as start for refined segments
      audioTimeMs: segment.audioTimeMs,
      confidence: 1.0, // AI-refined content has high confidence
      speaker: speakerPrefix,
      isFinal: true,
      isManuallyEdited: false,
      isAIRefined: true // Mark as AI-refined
    }));
  }

  /**
   * Transcribe audio file using Gemini Multimodal API
   * Gemini 1.5+ models can directly process audio files (mp3, wav, aac, webm, etc.)
   */
  public static async transcribeAudioWithGemini(
    apiKey: string,
    audioBlob: Blob,
    modelName: string, // e.g., "models/gemini-1.5-flash" or "models/gemini-2.0-flash-exp"
    onProgress?: (progress: number) => void
  ): Promise<TranscriptionResult[]> {
    if (!apiKey || apiKey.trim().length === 0) {
      throw new Error('Gemini API Key is required');
    }

    if (!modelName || !modelName.startsWith('models/')) {
      throw new Error('Please select a Gemini model in Settings');
    }

    if (onProgress) onProgress(10);

    try {
      // Convert WebM to WAV if needed (Gemini requires WAV or MP3)
      let processedAudio = audioBlob;
      if (audioBlob.type === 'audio/webm' || audioBlob.type === 'video/webm') {
        if (onProgress) onProgress(15);
        processedAudio = await this.convertToWav(audioBlob);
        if (onProgress) onProgress(25);
      }

      // Convert audio blob to base64
      const base64Audio = await this.blobToBase64(processedAudio);
      if (onProgress) onProgress(30);

      // Get MIME type (use WAV if converted)
      const mimeType = processedAudio.type || 'audio/wav';

      // Prepare request
      const endpoint = `https://generativelanguage.googleapis.com/${this.GEMINI_API_VERSION}/${modelName}:generateContent?key=${apiKey}`;

      const requestBody = {
        contents: [{
          parts: [
            {
              text: `HÃ£y nghe file Ã¢m thanh cuá»™c há»p nÃ y vÃ  chuyá»ƒn thÃ nh vÄƒn báº£n. YÃªu cáº§u báº¯t buá»™c:

1. Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n há»™i thoáº¡i tá»± nhiÃªn.
2. Gáº¯n nhÃ£n thá»i gian [mm:ss] vÃ o Ä‘áº§u má»—i Ä‘oáº¡n dá»±a trÃªn thá»i Ä‘iá»ƒm ngÆ°á»i nÃ³i báº¯t Ä‘áº§u trong file Ã¢m thanh.
3. Náº¿u cÃ³ nhiá»u ngÆ°á»i nÃ³i, hÃ£y phÃ¢n biá»‡t báº±ng cÃ¡ch ghi 'NgÆ°á»i nÃ³i 1:', 'NgÆ°á»i nÃ³i 2:'...
4. LÃ m sáº¡ch vÄƒn báº£n (loáº¡i bá» tá»« Ä‘á»‡m, sá»­a lá»—i chÃ­nh táº£).
5. Äá»‹nh dáº¡ng: Tráº£ vá» káº¿t quáº£ dÆ°á»›i dáº¡ng JSON vá»›i cáº¥u trÃºc:
{
  "segments": [
    {
      "timestamp": "mm:ss",
      "speaker": "NgÆ°á»i nÃ³i 1",
      "text": "ná»™i dung"
    }
  ]
}`
            },
            {
              inline_data: {
                mime_type: mimeType,
                data: base64Audio
              }
            }
          ]
        }]
      };

      if (onProgress) onProgress(40);

      // Make API request
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody)
      });

      if (onProgress) onProgress(70);

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(
          `Gemini API error (${response.status}): ${errorData.error?.message || response.statusText}`
        );
      }

      const data = await response.json();
      if (onProgress) onProgress(90);

      // Parse response
      const results = this.parseGeminiAudioTranscription(data);
      if (onProgress) onProgress(100);

      return results;

    } catch (error: any) {
      console.error('Gemini audio transcription error:', error);
      throw new Error(`Failed to transcribe audio: ${error.message}`);
    }
  }

  /**
   * Convert Blob to Base64 string
   */
  private static blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        const base64 = (reader.result as string).split(',')[1]; // Remove data:audio/...;base64, prefix
        resolve(base64);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  /**
   * Convert audio blob to WAV format
   * Gemini API requires WAV or MP3 format, not WebM
   */
  private static async convertToWav(audioBlob: Blob): Promise<Blob> {
    return new Promise((resolve, reject) => {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const reader = new FileReader();

      reader.onload = async (e) => {
        try {
          const arrayBuffer = e.target?.result as ArrayBuffer;
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

          // Convert to WAV
          const wavBlob = this.audioBufferToWav(audioBuffer);
          resolve(wavBlob);
        } catch (error) {
          reject(error);
        }
      };

      reader.onerror = reject;
      reader.readAsArrayBuffer(audioBlob);
    });
  }

  /**
   * Convert AudioBuffer to WAV Blob
   */
  private static audioBufferToWav(audioBuffer: AudioBuffer): Blob {
    const numberOfChannels = audioBuffer.numberOfChannels;
    const sampleRate = audioBuffer.sampleRate;
    const format = 1; // PCM
    const bitDepth = 16;

    const bytesPerSample = bitDepth / 8;
    const blockAlign = numberOfChannels * bytesPerSample;

    const data = [];
    for (let i = 0; i < audioBuffer.numberOfChannels; i++) {
      data.push(audioBuffer.getChannelData(i));
    }

    const interleaved = this.interleave(data);
    const dataLength = interleaved.length * bytesPerSample;
    const buffer = new ArrayBuffer(44 + dataLength);
    const view = new DataView(buffer);

    // Write WAV header
    this.writeString(view, 0, 'RIFF');
    view.setUint32(4, 36 + dataLength, true);
    this.writeString(view, 8, 'WAVE');
    this.writeString(view, 12, 'fmt ');
    view.setUint32(16, 16, true); // fmt chunk size
    view.setUint16(20, format, true);
    view.setUint16(22, numberOfChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * blockAlign, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bitDepth, true);
    this.writeString(view, 36, 'data');
    view.setUint32(40, dataLength, true);

    // Write audio data
    this.floatTo16BitPCM(view, 44, interleaved);

    return new Blob([buffer], { type: 'audio/wav' });
  }

  /**
   * Interleave multiple audio channels
   */
  private static interleave(channelData: Float32Array[]): Float32Array {
    const length = channelData[0].length;
    const numberOfChannels = channelData.length;
    const result = new Float32Array(length * numberOfChannels);

    let offset = 0;
    for (let i = 0; i < length; i++) {
      for (let channel = 0; channel < numberOfChannels; channel++) {
        result[offset++] = channelData[channel][i];
      }
    }

    return result;
  }

  /**
   * Write string to DataView
   */
  private static writeString(view: DataView, offset: number, string: string): void {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i));
    }
  }

  /**
   * Convert Float32 samples to 16-bit PCM
   */
  private static floatTo16BitPCM(view: DataView, offset: number, input: Float32Array): void {
    for (let i = 0; i < input.length; i++, offset += 2) {
      const s = Math.max(-1, Math.min(1, input[i]));
      view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
    }
  }

  /**
   * Parse Gemini audio transcription response
   */
  private static parseGeminiAudioTranscription(apiResponse: any): TranscriptionResult[] {
    try {
      const candidates = apiResponse.candidates;
      if (!candidates || candidates.length === 0) {
        throw new Error('No response from Gemini API');
      }

      const content = candidates[0].content;
      if (!content || !content.parts || content.parts.length === 0) {
        throw new Error('Empty response from Gemini');
      }

      const textResponse = content.parts[0].text;
      if (!textResponse) {
        throw new Error('No text in Gemini response');
      }

      // Extract JSON from response (handle markdown code blocks)
      let jsonText = textResponse.trim();
      const jsonMatch = jsonText.match(/```json\s*([\s\S]*?)```/) || jsonText.match(/```\s*([\s\S]*?)```/);
      if (jsonMatch) {
        jsonText = jsonMatch[1].trim();
      }

      const parsed = JSON.parse(jsonText);

      if (!parsed.segments || !Array.isArray(parsed.segments)) {
        throw new Error('Invalid JSON structure: missing segments array');
      }

      // Convert to TranscriptionResult format
      return parsed.segments.map((segment: any, index: number) => {
        const timestamp = segment.timestamp || '0:00';
        const audioTimeMs = this.parseTimestampToMs(timestamp);

        return {
          id: `gemini-${Date.now()}-${index}`,
          text: segment.text || '',
          startTime: new Date().toLocaleString('vi-VN'),
          endTime: new Date().toLocaleString('vi-VN'),
          audioTimeMs,
          confidence: 1.0,
          speaker: segment.speaker || 'Unknown',
          isFinal: true,
          isManuallyEdited: false,
          isAIRefined: true
        };
      });

    } catch (error: any) {
      console.error('Failed to parse Gemini audio transcription:', error);
      console.log('Raw API response:', JSON.stringify(apiResponse, null, 2));
      throw new Error(`Failed to parse Gemini response: ${error.message}`);
    }
  }

  /**
   * Parse timestamp string (mm:ss or m:ss) to milliseconds
   */
  private static parseTimestampToMs(timestamp: string): number {
    try {
      const parts = timestamp.split(':').map(p => parseInt(p.trim(), 10));
      if (parts.length === 2) {
        const [minutes, seconds] = parts;
        return (minutes * 60 + seconds) * 1000;
      }
      return 0;
    } catch {
      return 0;
    }
  }
}
