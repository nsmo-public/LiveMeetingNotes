import React, { useState, useEffect, useRef } from 'react';
import { MetadataPanel } from './components/MetadataPanel';
import { RecordingControls } from './components/RecordingControls';
import { NotesEditor } from './components/NotesEditor';
import { AudioPlayer, AudioPlayerRef } from './components/AudioPlayer';
import { HelpButton } from './components/HelpButton';
import { TranscriptionConfig } from './components/TranscriptionConfig';
import { TranscriptionPanel } from './components/TranscriptionPanel';
import { FileManagerService } from './services/fileManager';
import { saveBackup, loadBackup, clearBackup, hasBackup, getBackupAge } from './services/autoBackup';
import { speechToTextService, SpeechToTextService } from './services/speechToText';
import { AIRefinementService, type RawTranscriptData } from './services/aiRefinement';
import type { MeetingInfo, SpeechToTextConfig, TranscriptionResult } from './types/types';
import { message, Modal } from 'antd';
import { ExclamationCircleOutlined } from '@ant-design/icons';
import './styles/global.css';

export const App: React.FC = () => {
  const [folderPath, setFolderPath] = useState<string>('');
  const [isRecording, setIsRecording] = useState(false);
  const [audioBlob, setAudioBlob] = useState<Blob | null>(null);
  const [meetingInfo, setMeetingInfo] = useState<MeetingInfo>({
    title: `${new Date().toISOString().split('T')[0]} _ `,
    date: new Date().toISOString().split('T')[0],
    time: new Date().toTimeString().slice(0, 5),
    location: '',
    host: '',
    attendees: ''
  });
  const [notes, setNotes] = useState<string>('');
  const [timestampMap, setTimestampMap] = useState<Map<number, number>>(new Map());
  const [speakersMap, setSpeakersMap] = useState<Map<number, string>>(new Map());
  const [recordingStartTime, setRecordingStartTime] = useState<number>(0);
  const [hasUnsavedChanges, setHasUnsavedChanges] = useState(false);
  const [isSaved, setIsSaved] = useState(false);
  const [savedNotesSnapshot, setSavedNotesSnapshot] = useState<string>('');
  const [savedSpeakersSnapshot, setSavedSpeakersSnapshot] = useState<Map<number, string>>(new Map());
  const [isLiveMode, setIsLiveMode] = useState(true); // true = live recording, false = loaded project
  const [showBackupDialog, setShowBackupDialog] = useState(false);
  const [backupAge, setBackupAge] = useState<number | null>(null);
  const autoSaveTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const audioPlayerRef = useRef<AudioPlayerRef>(null);
  
  // Speech-to-Text states
  const [showTranscriptionConfig, setShowTranscriptionConfig] = useState(false);
  const [transcriptionConfig, setTranscriptionConfig] = useState<SpeechToTextConfig | null>(null);
  const [transcriptions, setTranscriptions] = useState<TranscriptionResult[]>([]);
  const [rawTranscripts, setRawTranscripts] = useState<RawTranscriptData[]>([]); // Raw data from Web Speech API
  const [isOnline, setIsOnline] = useState(navigator.onLine);

  // Check browser compatibility
  useEffect(() => {
    if (!FileManagerService.isSupported()) {
      console.warn(
        'File System Access API not supported. Files will be downloaded instead.'
      );
    }
  }, []);
  
  // Load Speech-to-Text config on mount
  useEffect(() => {
    const savedConfig = SpeechToTextService.loadConfig();
    if (savedConfig) {
      setTranscriptionConfig(savedConfig);
      speechToTextService.initialize(savedConfig);
      // console.log('üé§ Speech-to-Text config loaded');
    }
  }, []);
  
  // Monitor online/offline status
  useEffect(() => {
    const handleOnline = () => setIsOnline(true);
    const handleOffline = () => setIsOnline(false);
    
    window.addEventListener('online', handleOnline);
    window.addEventListener('offline', handleOffline);
    
    return () => {
      window.removeEventListener('online', handleOnline);
      window.removeEventListener('offline', handleOffline);
    };
  }, []);
  
  // Handle transcribe-audio event from AudioPlayer
  useEffect(() => {
    const handleTranscribeAudio = async () => {
      if (!audioBlob) {
        alert('No audio file loaded');
        return;
      }

      if (!speechToTextService.isConfigured()) {
        alert('Please configure Speech-to-Text settings first');
        setShowTranscriptionConfig(true);
        return;
      }

      const confirmed = window.confirm(
        'This will transcribe the entire audio file and replace existing transcription results. Continue?'
      );
      
      if (!confirmed) return;

      // console.log('üé¨ Starting audio file transcription...');
      
      // Clear existing transcriptions
      setTranscriptions([]);

      try {
        // Show progress notification
        const progressDiv = document.createElement('div');
        progressDiv.id = 'transcribe-progress';
        progressDiv.style.cssText = `
          position: fixed;
          top: 50%;
          left: 50%;
          transform: translate(-50%, -50%);
          background: white;
          padding: 24px;
          border-radius: 8px;
          box-shadow: 0 4px 12px rgba(0,0,0,0.3);
          z-index: 10000;
          min-width: 300px;
          text-align: center;
        `;
        progressDiv.innerHTML = `
          <div style="font-size: 16px; font-weight: bold; margin-bottom: 12px;">üé§ Transcribing Audio...</div>
          <div id="progress-text" style="font-size: 14px; color: #666;">Starting...</div>
          <div style="width: 100%; height: 8px; background: #f0f0f0; border-radius: 4px; margin-top: 12px; overflow: hidden;">
            <div id="progress-bar" style="width: 0%; height: 100%; background: #1890ff; transition: width 0.3s;"></div>
          </div>
        `;
        document.body.appendChild(progressDiv);

        const updateProgress = (progress: number) => {
          const progressBar = document.getElementById('progress-bar');
          const progressText = document.getElementById('progress-text');
          if (progressBar) progressBar.style.width = `${progress}%`;
          if (progressText) progressText.textContent = `${Math.floor(progress)}% complete`;
        };

        await speechToTextService.transcribeAudioFile(
          audioBlob,
          handleNewTranscription,
          updateProgress,
          () => {
            progressDiv.remove();
            setHasUnsavedChanges(true); // Mark as unsaved
            alert('‚úÖ Transcription complete! Click "Save Changes" to save the results.');
          }
        );
      } catch (error: any) {
        const progressDiv = document.getElementById('transcribe-progress');
        if (progressDiv) progressDiv.remove();
        alert(`Transcription failed: ${error.message}`);
        console.error('Transcription error:', error);
      }
    };

    window.addEventListener('transcribe-audio', handleTranscribeAudio);
    return () => {
      window.removeEventListener('transcribe-audio', handleTranscribeAudio);
    };
  }, [audioBlob, transcriptionConfig]);
  
  // Check for existing backup on mount
  useEffect(() => {
    const checkBackup = async () => {
      if (hasBackup()) {
        const age = getBackupAge();
        setBackupAge(age);
        setShowBackupDialog(true);
      }
    };
    checkBackup();
  }, []);

  // Track unsaved changes
  useEffect(() => {
    // Check if speakers have been modified
    const speakersModified = isSaved && !mapsAreEqual(savedSpeakersSnapshot, speakersMap);
    
    // C√≥ d·ªØ li·ªáu ch∆∞a l∆∞u n·∫øu:
    // 1. ƒêang recording
    // 2. C√≥ audio/notes nh∆∞ng ch∆∞a save l·∫ßn ƒë·∫ßu
    // 3. ƒê√£ save nh∆∞ng notes ho·∫∑c speakers b·ªã s·ª≠a ƒë·ªïi
    const notesModified = isSaved && savedNotesSnapshot !== notes;
    const hasData = isRecording || (!isSaved && (audioBlob !== null || notes.trim().length > 0)) || notesModified || speakersModified;
    setHasUnsavedChanges(hasData);
  }, [isRecording, audioBlob, notes, speakersMap, isSaved, savedNotesSnapshot, savedSpeakersSnapshot]);
  
  // Helper function to compare two Maps
  function mapsAreEqual(map1: Map<number, string>, map2: Map<number, string>): boolean {
    if (map1.size !== map2.size) return false;
    for (const [key, value] of map1) {
      if (map2.get(key) !== value) return false;
    }
    return true;
  }
  
  // Auto-save to localStorage with debounce (every 3 seconds after changes)
  useEffect(() => {
    // Auto-save whenever there are unsaved changes (including after first save)
    // Backup will be cleared only when user explicitly saves
    if (hasUnsavedChanges) {
      if (autoSaveTimeoutRef.current) {
        clearTimeout(autoSaveTimeoutRef.current);
      }
      
      autoSaveTimeoutRef.current = setTimeout(() => {
        const meetingInfoForBackup = {
          projectName: meetingInfo.title,
          location: meetingInfo.location,
          participants: meetingInfo.attendees
        };
        
        saveBackup(
          meetingInfoForBackup,
          notes,
          timestampMap,
          recordingStartTime,
          audioBlob,
          isSaved
        );
      }, 3000); // Auto-save 3 seconds after last change
    }
    
    return () => {
      if (autoSaveTimeoutRef.current) {
        clearTimeout(autoSaveTimeoutRef.current);
      }
    };
  }, [meetingInfo, notes, timestampMap, recordingStartTime, audioBlob, hasUnsavedChanges, isSaved]);

  // Switch to live mode when starting a new recording
  useEffect(() => {
    if (isRecording) {
      setIsLiveMode(true);
    }
  }, [isRecording]);

  // Debug: Log when meetingInfo changes
  useEffect(() => {
    // console.log('üìù App meetingInfo state updated:', meetingInfo);
  }, [meetingInfo]);

  // Prevent accidental page close/reload when recording or has unsaved data
  useEffect(() => {
    const handleBeforeUnload = (e: BeforeUnloadEvent) => {
      if (hasUnsavedChanges) {
        // Chu·∫©n modern browsers
        e.preventDefault();
        // Chrome requires returnValue to be set
        e.returnValue = 'B·∫°n c√≥ d·ªØ li·ªáu ch∆∞a l∆∞u. B·∫°n c√≥ ch·∫Øc mu·ªën r·ªùi kh·ªèi trang?';
        return e.returnValue;
      }
    };

    window.addEventListener('beforeunload', handleBeforeUnload);
    return () => window.removeEventListener('beforeunload', handleBeforeUnload);
  }, [hasUnsavedChanges]);

  // Clear unsaved changes flag after successful save
  const handleAudioBlobChange = (blob: Blob | null) => {
    setAudioBlob(blob);
  };

  const handleSaveComplete = () => {
    setIsSaved(true);
    setHasUnsavedChanges(false);
    setSavedNotesSnapshot(notes); // Save snapshot to detect future changes
    setSavedSpeakersSnapshot(new Map(speakersMap)); // Save speakers snapshot
    // Clear auto-backup after successful save
    clearBackup();
  };
  
  const handleRestoreBackup = async () => {
    const backup = await loadBackup();
    if (backup) {
      setMeetingInfo({
        title: backup.meetingInfo.projectName,
        date: new Date().toISOString().split('T')[0],
        time: new Date().toTimeString().slice(0, 5),
        location: backup.meetingInfo.location,
        host: '',
        attendees: backup.meetingInfo.participants
      });
      setNotes(backup.notes);
      setTimestampMap(backup.timestampMap);
      setRecordingStartTime(backup.recordingStartTime);
      if (backup.audioBlob) {
        setAudioBlob(backup.audioBlob);
      }
      setIsSaved(backup.isSaved);
      setShowBackupDialog(false);
      // console.log('‚úÖ Backup restored successfully');
    }
  };
  
  const handleDiscardBackup = async () => {
    await clearBackup();
    setShowBackupDialog(false);
    // console.log('üóëÔ∏è Backup discarded');
  };

  const handleLoadProject = (loadedData: {
    meetingInfo: MeetingInfo;
    notes: string;
    timestampMap: Map<number, number>;
    speakersMap: Map<number, string>;
    audioBlob: Blob | null;
    recordingStartTime: number;
    transcriptions?: TranscriptionResult[]; // Add transcriptions array
    rawTranscripts?: RawTranscriptData[]; // Add raw transcripts for AI refinement
  }) => {
    // console.log('üìÇ App.handleLoadProject - Data received:', {
    //   meetingInfo: loadedData.meetingInfo,
    //   notesLength: loadedData.notes.length,
    //   timestampMapSize: loadedData.timestampMap.size,
    //   speakersMapSize: loadedData.speakersMap.size,
    //   audioBlobSize: loadedData.audioBlob?.size || 0,
    //   hasAudio: loadedData.audioBlob !== null,
    //   transcriptionsCount: loadedData.transcriptions?.length || 0
    // });
    
    setMeetingInfo(loadedData.meetingInfo);
    setNotes(loadedData.notes);
    setTimestampMap(loadedData.timestampMap);
    setSpeakersMap(loadedData.speakersMap);
    setAudioBlob(loadedData.audioBlob);
    setRecordingStartTime(loadedData.recordingStartTime);
    
    // Load transcriptions if available
    if (loadedData.transcriptions && loadedData.transcriptions.length > 0) {
      setTranscriptions(loadedData.transcriptions);
    } else {
      setTranscriptions([]); // Clear transcriptions if none
    }
    
    // Load raw transcripts if available
    if (loadedData.rawTranscripts && loadedData.rawTranscripts.length > 0) {
      setRawTranscripts(loadedData.rawTranscripts);
    } else {
      setRawTranscripts([]); // Clear raw transcripts if none
    }
    
    setIsSaved(true);
    setHasUnsavedChanges(false);
    setSavedNotesSnapshot(loadedData.notes);
    setSavedSpeakersSnapshot(new Map(loadedData.speakersMap)); // Save speakers snapshot
    setIsLiveMode(false); // Switch to timestamp mode when loading project
  };

  // Handle transcription config save
  const handleSaveTranscriptionConfig = (config: SpeechToTextConfig) => {
    setTranscriptionConfig(config);
    speechToTextService.initialize(config);
    // console.log('‚úÖ Transcription config updated');
  };

  // Handle edit transcription
  const handleEditTranscription = (id: string, newText: string, newSpeaker: string, newStartTime?: string, newAudioTimeMs?: number) => {
    // Check if user deleted all text (wants to remove segment)
    if (!newText || newText.trim() === '') {
      const confirmed = window.confirm(
        '‚ö†Ô∏è B·∫°n ƒë√£ x√≥a to√†n b·ªô n·ªôi dung.\n\n' +
        'B·∫°n c√≥ mu·ªën x√≥a segment n√†y kh√¥ng?\n\n' +
        '‚Ä¢ ƒê·ªìng √Ω: X√≥a segment n√†y kh·ªèi danh s√°ch\n' +
        '‚Ä¢ H·ªßy: Gi·ªØ nguy√™n segment (kh√¥ng l∆∞u thay ƒë·ªïi)'
      );
      
      if (confirmed) {
        // Remove the segment
        setTranscriptions(prev => prev.filter(item => item.id !== id));
        setHasUnsavedChanges(true);
        message.success('ƒê√£ x√≥a segment');
        // console.log('üóëÔ∏è Transcription segment deleted:', id);
      }
      // If not confirmed, do nothing (keep original segment)
      return;
    }

    // Normal edit: update text and other fields
    setTranscriptions(prev => 
      prev.map(item => 
        item.id === id 
          ? { 
              ...item, 
              text: newText, 
              speaker: newSpeaker, 
              startTime: newStartTime !== undefined ? newStartTime : item.startTime,
              audioTimeMs: newAudioTimeMs !== undefined ? newAudioTimeMs : item.audioTimeMs,
              isManuallyEdited: true 
            }
          : item
      )
    );
    setHasUnsavedChanges(true);
    // console.log('‚úèÔ∏è Transcription edited:', { id, newText, newSpeaker, newStartTime, newAudioTimeMs });
  };

  // Handle new transcription result
  const handleNewTranscription = (result: TranscriptionResult) => {
    // Validate result has text
    if (!result || !result.text) {
      console.warn('‚ö†Ô∏è Received invalid transcription result:', result);
      return;
    }

    // Collect raw transcript data for AI refinement
    const rawData: RawTranscriptData = {
      text: result.text,
      timestamp: result.startTime,
      audioTimeMs: result.audioTimeMs,
      confidence: result.confidence,
      isFinal: result.isFinal
    };
    setRawTranscripts(prev => [...prev, rawData]);

    setTranscriptions(prev => {
      // N·∫øu l√† k·∫øt qu·∫£ final
      if (result.isFinal) {
        // Lo·∫°i b·ªè k·∫øt qu·∫£ t·∫°m th·ªùi (n·∫øu c√≥)
        const finalResults = prev.filter(item => item.isFinal);
        
        // Ki·ªÉm tra xem k·∫øt qu·∫£ m·ªõi c√≥ ph·∫£i l√† phi√™n b·∫£n m·ªü r·ªông c·ªßa k·∫øt qu·∫£ c≈© kh√¥ng
        // N·∫øu k·∫øt qu·∫£ cu·ªëi c√πng ch·ª©a h·∫ßu h·∫øt text c·ªßa k·∫øt qu·∫£ m·ªõi ho·∫∑c ng∆∞·ª£c l·∫°i
        if (finalResults.length > 0) {
          const lastResult = finalResults[finalResults.length - 1];
          
          // Validate both texts exist
          if (!lastResult.text) {
            // If last result has no text, replace it with new result
            finalResults[finalResults.length - 1] = result;
            return finalResults;
          }
          
          const newText = result.text.trim().toLowerCase();
          const lastText = lastResult.text.trim().toLowerCase();
          
          // Case 1: K·∫øt qu·∫£ m·ªõi l√† phi√™n b·∫£n m·ªü r·ªông c·ªßa k·∫øt qu·∫£ c≈©
          // VD: C≈©: "nh∆∞ v·∫≠y l√†", M·ªõi: "nh∆∞ v·∫≠y l√† c√°i m·∫´u"
          if (newText.startsWith(lastText) && newText.length > lastText.length) {
            // console.log('üîÑ Replacing with extended version:', {
            //   old: lastText.substring(0, 50) + '...',
            //   new: newText.substring(0, 50) + '...'
            // });
            // Thay th·∫ø k·∫øt qu·∫£ c≈© b·∫±ng k·∫øt qu·∫£ m·ªõi
            finalResults[finalResults.length - 1] = result;
            return finalResults;
          }
          
          // Case 2: K·∫øt qu·∫£ c≈© l√† phi√™n b·∫£n m·ªü r·ªông c·ªßa k·∫øt qu·∫£ m·ªõi ‚Üí b·ªè qua k·∫øt qu·∫£ m·ªõi
          // VD: C≈©: "nh∆∞ v·∫≠y l√† c√°i m·∫´u", M·ªõi: "nh∆∞ v·∫≠y l√†"
          if (lastText.startsWith(newText)) {
            // console.log('‚è≠Ô∏è Skipping shorter duplicate');
            return prev; // Gi·ªØ nguy√™n
          }
          
          // Case 3: Ki·ªÉm tra ƒë·ªô t∆∞∆°ng ƒë·ªìng cao (>80% gi·ªëng nhau)
          const similarity = calculateSimilarity(newText, lastText);
          if (similarity > 0.8) {
            // console.log('‚è≠Ô∏è Skipping similar result (similarity: ' + (similarity * 100).toFixed(0) + '%)');
            return prev;
          }
        }
        
        // Th√™m k·∫øt qu·∫£ m·ªõi
        return [...finalResults, result];
      } else {
        // N·∫øu l√† k·∫øt qu·∫£ t·∫°m th·ªùi, ch·ªâ gi·ªØ 1 k·∫øt qu·∫£ t·∫°m th·ªùi m·ªõi nh·∫•t
        const finalResults = prev.filter(item => item.isFinal);
        return [...finalResults, result];
      }
    });
    
    if (result.isFinal) {
      // console.log('‚úÖ Final transcription:', result.text.substring(0, 50) + '...');
    }
  };

  // Calculate text similarity (Levenshtein-based)
  const calculateSimilarity = (text1: string, text2: string): number => {
    const longer = text1.length > text2.length ? text1 : text2;
    const shorter = text1.length > text2.length ? text2 : text1;
    
    if (longer.length === 0) return 1.0;
    
    // Quick check: if one contains the other
    if (longer.includes(shorter)) {
      return shorter.length / longer.length;
    }
    
    // Simple word-based similarity
    const words1 = text1.split(/\s+/);
    const words2 = text2.split(/\s+/);
    const commonWords = words1.filter(w => words2.includes(w)).length;
    
    return (2 * commonWords) / (words1.length + words2.length);
  };

  // Handle seek to audio time
  const handleSeekToAudio = (timeMs: number) => {
    if (audioPlayerRef.current) {
      audioPlayerRef.current.seekTo(timeMs);
      // console.log(`‚è≠Ô∏è Seeking to ${(timeMs / 1000).toFixed(2)}s`);
    }
  };

  // Handle AI refinement
  const handleAIRefine = async () => {
    if (!transcriptionConfig) {
      message.warning('Vui l√≤ng c·∫•u h√¨nh Speech-to-Text Settings tr∆∞·ªõc');
      setShowTranscriptionConfig(true);
      return;
    }

    // Check for Gemini API key
    const apiKeyToUse = transcriptionConfig.geminiApiKey || transcriptionConfig.apiKey;
    if (!apiKeyToUse) {
      message.error({
        content: (
          <div>
            <div style={{ fontWeight: 'bold', marginBottom: '8px' }}>
              C·∫ßn Gemini API Key ƒë·ªÉ s·ª≠ d·ª•ng t√≠nh nƒÉng AI
            </div>
            <div style={{ fontSize: '13px', lineHeight: '1.6' }}>
              <strong>C√°ch l·∫•y API Key mi·ªÖn ph√≠:</strong>
              <ol style={{ paddingLeft: '20px', margin: '8px 0' }}>
                <li>Truy c·∫≠p: <a href="https://aistudio.google.com/app/apikey" target="_blank">Google AI Studio</a></li>
                <li>Click "Create API Key"</li>
                <li>Copy API key v√† paste v√†o Settings ‚Üí Gemini API Key</li>
                <li>H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông t·∫£i danh s√°ch models</li>
                <li>Ch·ªçn model (khuy√™n d√πng: Gemini 2.5 Flash)</li>
              </ol>
            </div>
          </div>
        ),
        duration: 10
      });
      setShowTranscriptionConfig(true);
      return;
    }

    // Check for model selection
    const selectedModel = transcriptionConfig.geminiModel;
    if (!selectedModel || !selectedModel.startsWith('models/')) {
      message.error({
        content: (
          <div>
            <div style={{ fontWeight: 'bold', marginBottom: '8px' }}>
              Vui l√≤ng ch·ªçn Gemini Model trong Settings
            </div>
            <div style={{ fontSize: '13px', lineHeight: '1.6' }}>
              <strong>C√°c b∆∞·ªõc:</strong>
              <ol style={{ paddingLeft: '20px', margin: '8px 0' }}>
                <li>M·ªü Settings</li>
                <li>Nh·∫≠p Gemini API Key (n·∫øu ch∆∞a c√≥)</li>
                <li>ƒê·ª£i h·ªá th·ªëng t·∫£i danh s√°ch models</li>
                <li>Ch·ªçn model t·ª´ dropdown (khuy√™n d√πng: Gemini 2.5 Flash)</li>
                <li>L∆∞u v√† th·ª≠ l·∫°i</li>
              </ol>
            </div>
          </div>
        ),
        duration: 10
      });
      setShowTranscriptionConfig(true);
      return;
    }

    if (transcriptions.length === 0) {
      message.warning('Kh√¥ng c√≥ d·ªØ li·ªáu chuy·ªÉn ƒë·ªïi ƒë·ªÉ chu·∫©n h√≥a');
      return;
    }

    // Show warning modal with better design
    Modal.confirm({
      title: (
        <div style={{ fontSize: '18px', fontWeight: 'bold', color: '#1890ff' }}>
          ü§ñ Chu·∫©n h√≥a vƒÉn b·∫£n b·∫±ng Gemini AI
        </div>
      ),
      icon: <ExclamationCircleOutlined style={{ color: '#1890ff' }} />,
      width: 680,
      content: (
        <div style={{ fontSize: '14px', lineHeight: '1.8' }}>
          <div style={{ marginBottom: '16px' }}>
            <div style={{ fontWeight: 'bold', marginBottom: '8px', color: '#52c41a' }}>
              ‚ú® AI s·∫Ω th·ª±c hi·ªán:
            </div>
            <ul style={{ paddingLeft: '20px', margin: '0' }}>
              <li>S·ª≠a l·ªói nh·∫≠n di·ªán t·ª´ Web Speech API</li>
              <li>Lo·∫°i b·ªè t·ª´ th·ª´a, t·ª´ ƒë·ªám (√†, ·ª´m, th√¨...)</li>
              <li>Th√™m d·∫•u c√¢u v√† vi·∫øt hoa ƒë√∫ng quy t·∫Øc</li>
              <li>G·ªôp c√°c ƒëo·∫°n li√™n quan th√†nh c√¢u ho√†n ch·ªânh</li>
            </ul>
          </div>

          <div style={{ 
            background: '#fff7e6', 
            border: '2px solid #ffa940',
            borderRadius: '8px',
            padding: '16px',
            marginBottom: '16px'
          }}>
            <div style={{ 
              fontWeight: 'bold', 
              marginBottom: '12px', 
              color: '#fa8c16',
              fontSize: '15px',
              display: 'flex',
              alignItems: 'center',
              gap: '8px'
            }}>
              <span style={{ fontSize: '20px' }}>‚ö†Ô∏è</span>
              C·∫¢NH B√ÅO QUAN TR·ªåNG V·ªÄ B·∫¢O M·∫¨T
            </div>
            
            <div style={{ marginBottom: '12px', color: '#595959' }}>
              D·ªØ li·ªáu c·ªßa b·∫°n s·∫Ω ƒë∆∞·ª£c <strong>g·ª≠i ƒë·∫øn Google Gemini API</strong> ƒë·ªÉ x·ª≠ l√Ω.
            </div>

            <div style={{ 
              background: '#fff1f0',
              border: '1px solid #ffccc7',
              borderRadius: '6px',
              padding: '12px',
              marginBottom: '12px'
            }}>
              <div style={{ fontWeight: 'bold', marginBottom: '8px', color: '#cf1322' }}>
                üö´ KH√îNG s·ª≠ d·ª•ng v·ªõi th√¥ng tin nh·∫°y c·∫£m
              </div>
              <ul style={{ paddingLeft: '20px', margin: '0', color: '#595959' }}>
                <li><strong>T√†i ch√≠nh:</strong> M·∫≠t kh·∫©u, s·ªë t√†i kho·∫£n, s·ªë th·∫ª, giao d·ªãch ng√¢n h√†ng</li>
                <li><strong>Y t·∫ø:</strong> B·ªánh √°n, ƒë∆°n thu·ªëc, k·∫øt qu·∫£ x√©t nghi·ªám</li>
                <li><strong>C√° nh√¢n:</strong> CCCD/CMND, ƒë·ªãa ch·ªâ, s·ªë ƒëi·ªán tho·∫°i nh·∫°y c·∫£m</li>
                <li><strong>Doanh nghi·ªáp:</strong> B√≠ m·∫≠t th∆∞∆°ng m·∫°i, k·∫ø ho·∫°ch kinh doanh, c√°c n·ªôi dung m·∫≠t kh√°c</li>
                <li><strong>B·∫£o m·∫≠t:</strong> API keys, tokens, credentials</li>
              </ul>
            </div>

            <div style={{ 
              fontStyle: 'italic', 
              color: '#8c8c8c',
              fontSize: '13px'
            }}>
              üí° Khuy·∫øn ngh·ªã: H√£y xem l·∫°i n·ªôi dung transcript tr∆∞·ªõc khi s·ª≠ d·ª•ng ch·ª©c nƒÉng n√†y
            </div>
          </div>

          <div style={{ 
            background: '#e6f7ff',
            border: '1px solid #91d5ff',
            borderRadius: '6px',
            padding: '12px',
            fontSize: '13px',
            color: '#595959'
          }}>
            <strong>‚ÑπÔ∏è L∆∞u √Ω:</strong> Qu√° tr√¨nh n√†y s·∫Ω thay th·∫ø to√†n b·ªô k·∫øt qu·∫£ hi·ªán t·∫°i. 
            B·∫°n c√≥ th·ªÉ ch·ªânh s·ª≠a l·∫°i sau n·∫øu c·∫ßn.
          </div>
        </div>
      ),
      okText: 'ƒê·ªìng √Ω, ti·∫øp t·ª•c',
      cancelText: 'H·ªßy b·ªè',
      okButtonProps: {
        danger: false,
        type: 'primary'
      },
      onOk: async () => {
        await performAIRefinement();
      }
    });
  };

  // Separate function to perform AI refinement
  const performAIRefinement = async () => {
    const apiKeyToUse = transcriptionConfig!.geminiApiKey || transcriptionConfig!.apiKey;
    const selectedModel = transcriptionConfig!.geminiModel;

    try {
      // Show progress dialog
      const progressDiv = document.createElement('div');
      progressDiv.id = 'ai-refine-progress';
      progressDiv.style.cssText = `
        position: fixed;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        background: white;
        padding: 24px;
        border-radius: 8px;
        box-shadow: 0 4px 12px rgba(0,0,0,0.3);
        z-index: 10000;
        min-width: 350px;
        text-align: center;
      `;
      progressDiv.innerHTML = `
        <div style="font-size: 18px; font-weight: bold; margin-bottom: 12px;">ü§ñ AI ƒëang chu·∫©n h√≥a vƒÉn b·∫£n...</div>
        <div id="ai-progress-text" style="font-size: 14px; color: #666;">ƒêang x·ª≠ l√Ω...</div>
        <div style="width: 100%; height: 8px; background: #f0f0f0; border-radius: 4px; margin-top: 12px; overflow: hidden;">
          <div id="ai-progress-bar" style="width: 0%; height: 100%; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); transition: width 0.3s;"></div>
        </div>
      `;
      document.body.appendChild(progressDiv);

      const updateProgress = (progress: number) => {
        const progressBar = document.getElementById('ai-progress-bar');
        const progressText = document.getElementById('ai-progress-text');
        if (progressBar) progressBar.style.width = `${progress}%`;
        if (progressText) {
          if (progress < 20) progressText.textContent = 'ƒêang chu·∫©n b·ªã d·ªØ li·ªáu...';
          else if (progress < 70) progressText.textContent = 'ƒêang g·ª≠i ƒë·∫øn AI...';
          else if (progress < 90) progressText.textContent = 'ƒêang x·ª≠ l√Ω k·∫øt qu·∫£...';
          else progressText.textContent = 'Ho√†n th√†nh!';
        }
      };

      // Prepare raw data for supplementary reference
      let rawData: RawTranscriptData[] = [];
      if (rawTranscripts && rawTranscripts.length > 0) {
        // Use saved raw data (preserves original Web Speech API output)
        rawData = rawTranscripts;
        console.log('üì¶ Using saved raw transcripts as supplementary data:', rawData.length, 'items');
      } else {
        // No raw data available - AI will only use transcriptions
        console.log('‚ÑπÔ∏è No raw data available - using transcriptions only');
      }

      // Call AI refinement service with model selection
      // Primary data: transcriptions (user-edited, highest reliability)
      // Supplementary data: rawTranscripts (original Web Speech API output for reference)
      const refinedSegments = await AIRefinementService.refineTranscripts(
        apiKeyToUse,
        transcriptions, // Primary data
        rawData, // Supplementary data
        selectedModel, // Pass required model name
        updateProgress
      );

      // Convert to TranscriptionResult format
      const refinedResults = AIRefinementService.convertToTranscriptionResults(
        refinedSegments,
        'Person1'
      );

      // Update transcriptions
      setTranscriptions(refinedResults);
      setHasUnsavedChanges(true);

      // Remove progress dialog
      progressDiv.remove();

      message.success(`‚úÖ ƒê√£ chu·∫©n h√≥a th√†nh c√¥ng ${refinedResults.length} ƒëo·∫°n vƒÉn b·∫£n!`);

    } catch (error: any) {
      const progressDiv = document.getElementById('ai-refine-progress');
      if (progressDiv) progressDiv.remove();

      console.error('AI Refinement Error:', error);
      message.error(`L·ªói khi chu·∫©n h√≥a b·∫±ng AI: ${error.message}`);
    }
  };

  return (
    <div className="app-container">
      {/* Backup Restoration Dialog */}
      {showBackupDialog && (
        <div style={{
          position: 'fixed',
          top: 0,
          left: 0,
          right: 0,
          bottom: 0,
          backgroundColor: 'rgba(0, 0, 0, 0.7)',
          display: 'flex',
          alignItems: 'center',
          justifyContent: 'center',
          zIndex: 9999
        }}>
          <div style={{
            backgroundColor: '#1e1e1e',
            border: '2px solid #ffa500',
            borderRadius: '8px',
            padding: '24px',
            maxWidth: '500px',
            boxShadow: '0 4px 16px rgba(0, 0, 0, 0.5)'
          }}>
            <h2 style={{ marginTop: 0, color: '#ffa500' }}>üîÑ Kh√¥i ph·ª•c d·ªØ li·ªáu</h2>
            <p style={{ fontSize: '16px', lineHeight: '1.6' }}>
              Ph√°t hi·ªán d·ªØ li·ªáu t·ª± ƒë·ªông sao l∆∞u t·ª´ <strong>{backupAge !== null ? `${backupAge} ph√∫t` : 'm·ªôt l√∫c'}</strong> tr∆∞·ªõc.
              <br/>
              C√≥ th·ªÉ tr√¨nh duy·ªát ƒë√£ b·ªã ƒë√≥ng ƒë·ªôt ng·ªôt ho·∫∑c b·∫°n ch∆∞a l∆∞u d·ªØ li·ªáu.
            </p>
            <p style={{ fontSize: '14px', color: '#888' }}>
              B·∫°n c√≥ mu·ªën kh√¥i ph·ª•c d·ªØ li·ªáu n√†y kh√¥ng?
            </p>
            <div style={{ display: 'flex', gap: '12px', marginTop: '20px' }}>
              <button
                onClick={handleRestoreBackup}
                style={{
                  flex: 1,
                  padding: '12px 20px',
                  backgroundColor: '#1890ff',
                  color: 'white',
                  border: 'none',
                  borderRadius: '6px',
                  fontSize: '16px',
                  fontWeight: 'bold',
                  cursor: 'pointer'
                }}
              >
                ‚úÖ Kh√¥i ph·ª•c
              </button>
              <button
                onClick={handleDiscardBackup}
                style={{
                  flex: 1,
                  padding: '12px 20px',
                  backgroundColor: '#434343',
                  color: 'white',
                  border: 'none',
                  borderRadius: '6px',
                  fontSize: '16px',
                  cursor: 'pointer'
                }}
              >
                üóëÔ∏è B·ªè qua
              </button>
            </div>
          </div>
        </div>
      )}
      
      <header className="app-header">
        <h1>üìù Live Meeting Notes</h1>
        <div className="status-indicator">
          {navigator.onLine ? 'üåê Online' : 'üì¥ Offline'}
          {hasUnsavedChanges && <span className="unsaved-indicator" title="B·∫°n c√≥ d·ªØ li·ªáu ch∆∞a l∆∞u">‚ö†Ô∏è Ch∆∞a l∆∞u</span>}
          <HelpButton />
        </div>
      </header>

      <MetadataPanel meetingInfo={meetingInfo} onChange={setMeetingInfo} />

      <RecordingControls
        folderPath={folderPath}
        onFolderSelect={setFolderPath}
        isRecording={isRecording}
        onRecordingChange={setIsRecording}
        onAudioBlobChange={handleAudioBlobChange}
        onSaveComplete={handleSaveComplete}
        onLoadProject={handleLoadProject}
        meetingInfo={meetingInfo}
        notes={notes}
        timestampMap={timestampMap}
        speakersMap={speakersMap}
        recordingStartTime={recordingStartTime}
        onRecordingStartTimeChange={setRecordingStartTime}
        audioBlob={audioBlob}
        isSaved={isSaved}
        hasUnsavedChanges={hasUnsavedChanges}
        onShowTranscriptionConfig={() => setShowTranscriptionConfig(true)}
        transcriptionConfig={transcriptionConfig}
        shouldBlink={!transcriptionConfig} 
        onNewTranscription={handleNewTranscription}
        onClearTranscriptions={() => {
          setTranscriptions([]);
          setRawTranscripts([]); // Also clear raw transcripts
        }}
        transcriptions={transcriptions}
      />

      {/* Transcription Panel - Only show when online and configured */}
      {isOnline && transcriptionConfig && (
        <TranscriptionPanel
          transcriptions={transcriptions}
          isTranscribing={isRecording}
          isOnline={isOnline}
          onSeekAudio={handleSeekToAudio}
          onEditTranscription={handleEditTranscription}
          onAIRefine={handleAIRefine}
          canRefineWithAI={
            !isRecording && 
            transcriptions.length > 0 && 
            (!!transcriptionConfig.geminiApiKey || !!transcriptionConfig.apiKey) &&
            !!transcriptionConfig.geminiModel
          }
        />
      )}

      <NotesEditor
        notes={notes}
        onNotesChange={setNotes}
        timestampMap={timestampMap}
        onTimestampMapChange={setTimestampMap}
        recordingStartTime={recordingStartTime}
        isLiveMode={isLiveMode}
        onSpeakersChange={setSpeakersMap}
        initialSpeakers={speakersMap}
      />

      <AudioPlayer ref={audioPlayerRef} audioBlob={audioBlob} transcriptionConfig={transcriptionConfig} />

      {/* Transcription Configuration Modal */}
      <TranscriptionConfig
        visible={showTranscriptionConfig}
        onClose={() => setShowTranscriptionConfig(false)}
        onSave={handleSaveTranscriptionConfig}
        currentConfig={transcriptionConfig}
      />
    </div>
  );
};
